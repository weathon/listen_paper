["Putting the Object Back into Video Object Segmentation", [{"title": "Section 0011", "content": "Putting the Object Back into Video Object Segmentation\nHo Kei Cheng1Seoung Wug Oh2Brian Price2Joon-Young Lee2Alexander Schwing1\n1University of Illinois Urbana-Champaign2Adobe Research\n{hokeikc2,aschwing}@illinois.edu, {seoh,bprice,jolee}@adobe.com\nAbstract\nWe present Cutie, a video object segmentation (VOS) net-\nwork with object-level memory reading, which puts the object\nrepresentation from memory back into the video object seg-\nmentation result. Recent works on VOS employ bottom-up\npixel-level memory reading which struggles due to matching\nnoise, especially in the presence of distractors, resulting\nin lower performance in more challenging data. In con-\ntrast, Cutie performs top-down object-level memory reading\nby adapting a small set of object queries. Via those, it in-\nteracts with the bottom-up pixel features iteratively with a\nquery-based object transformer ( qt, hence Cutie). The ob-\nject queries act as a high-level summary of the target object,\nwhile high-resolution feature maps are retained for accu-\nrate segmentation. Together with foreground-background\nmasked attention, Cutie cleanly separates the semantics of\nthe foreground object from the background. On the challeng-\ning MOSE dataset, Cutie improves by 8.7 J&Fover XMem\nwith a similar running time and improves by 4.2 J&Fover\nDeAOT while being three times faster. Code is available at:\nhkchengrex.github.io/Cutie .\n1. Introduction\nVideo Object Segmentation (VOS), specifically the \u201csemi-\nsupervised\u201d setting, requires tracking and segmenting objects\nfrom an open vocabulary specified in a first-frame annota-\ntion. VOS methods are broadly applicable in robotics [ 1],\nvideo editing [ 2], reducing costs in data annotation [ 3],\nand can also be combined with Segment Anything Models\n(SAMs) [ 4] for universal video segmentation (e.g., Tracking\nAnything [5\u20137]).\nRecent VOS approaches employ a memory-based\nparadigm [ 8\u201311]. "}, {"title": "Section 0024", "content": "A memory representation is computed\nfrom past segmented frames (either given as input or seg-\nmented by the model), and any new query frame \u201creads\u201d\nfrom this memory to retrieve features for segmentation. Im-\nportantly, these approaches mainly use pixel-level matching\nfor memory reading, either with one [ 8] or multiple matching\nlayers [ 10], and generate the segmentation bottom-up from\nFigure 1. Comparison of pixel-level memory reading v.s. object-\nlevel memory reading. In each box, the left is the reference frame,\nand the right is the query frame to be segmented. Red arrows indi-\ncate wrong matches. Low-level pixel matching (e.g., XMem [ 9])\ncan be noisy in the presence of distractors. We propose object-level\nmemory reading for more robust video object segmentation.\nthe pixel memory readout. Pixel-level matching maps every\nquery pixel independently to a linear combination of memory\npixels (e.g., with an attention layer). Consequently, pixel-\nlevel matching lacks high-level consistency and is prone\nto matching noise, especially in the presence of distractors.\nThis leads to lower performance in challenging scenarios\nwith occlusions and frequent distractors. Concretely, the\nperformance of recent approaches [ 9,10] is more than 20\npoints in J&Flower when evaluating on the recently pro-\nposed challenging MOSE [ 12] dataset rather than the simpler\nDA VIS-2017 [13] dataset.\nWe think this unsatisfactory result in challenging sce-\nnarios is caused by the lack of object-level reasoning. To\naddress this, we propose object-level memory reading , which\neffectively puts the object from a memory back into the query\nframe (Figure 1). Inspired by recent query-based object de-\ntection/segmentation [ 14\u201318] that represent objects as \u201cob-\nject queries,\u201d we implement our object-level memory reading\nwith an object transformer. "}, {"title": "Section 0030", "content": "This object transformer uses a\nsmall set of end-to-end trained object queries to 1) iteratively\nprobe and calibrate a feature map (initialized by a pixel-level\n1arXiv:2310.12982v2  [cs.CV]  11 Apr 2024memory readout), and 2) encode object-level information.\nThis approach simultaneously keeps a high-level/global ob-\nject query representation and a low-level/high-resolution fea-\nture map, enabling bidirectional top-down/bottom-up com-\nmunication. This communication is parameterized with a se-\nquence of attention layers, including a proposed foreground-\nbackground masked attention . The masked attention, ex-\ntended from foreground-only masked attention [ 15], lets part\nof the object queries attend only to the foreground while\nthe remainders attend only to the background \u2013 allowing\nboth global feature interaction and clean separation of fore-\nground/background semantics. Moreover, we introduce a\ncompact object memory (in addition to a pixel memory) to\nsummarize the features of target objects, enhancing end-to-\nend object queries with target-specific features.\nIn experiments, the proposed approach, Cutie , is sig-\nnificantly more robust in challenging scenarios (e.g., +8.7\nJ&Fin MOSE [ 12] over XMem [ 9]) than existing ap-\nproaches while remaining competitive in standard datasets\n(i.e., DA VIS [ 13] and YouTubeVOS [ 19]) in both accuracy\nand efficiency. In summary,\n\u2022We develop Cutie , which uses high-level top-down queries\nwith pixel-level bottom-up features for robust video object\nsegmentation in challenging scenarios.\n\u2022We extend masked attention to include foreground and\nbackground for both rich features and a clean semantic\nseparation between the target object and distractors.\n\u2022We construct a compact object memory to summarize ob-\nject features in the long term, which are retrieved as target-\nspecific object-level representations during querying.\n2. Related Works\nMemory-Based VOS. "}, {"title": "Section 0042", "content": "Since semi-supervised Video Ob-\nject Segmentation (VOS) involves a directional propagation\nof information, many existing approaches employ a feature\nmemory representation that stores past features for segment-\ning future frames. This includes online learning that fine-\ntunes a network on the first-frame segmentation for every\nvideo during inference [ 20\u201324]. However, finetuning is slow\nduring test-time. Recurrent approaches [ 25\u201331] are faster\nbut lack context for tracking under occlusion. Recent ap-\nproaches use more context [ 5,8,11,32\u201364] via pixel-level\nfeature matching and integration, with some exploring the\nmodeling of background features \u2013 either explicitly [ 36,65]\nor implicitly [ 51]. XMem [ 9] uses multiple types of memory\nfor better performance and efficiency but still struggles with\nnoise from low-level pixel matching. While we adopt the\nmemory reading of XMem [ 9], we develop an object reading\nmechanism to integrate the pixel features at an object level\nwhich permits Cutie to attain much better performance in\nchallenging scenarios.\nTransformers in VOS. Transformer-based [ 66] approacheshave been developed for pixel matching with memory in\nvideo object segmentation [ 10,50,53,67\u201370]. However,\nthey compute attention between spatial feature maps (as\ncross-attention, self-attention, or both), which is computa-\ntionally expensive with O(n4)time/space complexity, where\nnis the image side length. SST [ 67] proposes sparse atten-\ntion but performs worse than state-of-the-art methods. AOT\napproaches [ 10,68] use an identity bank for processing mul-\ntiple objects in a single forward pass to improve efficiency,\nbut are not permutation equivariant with respect to object\nID and do not scale well to longer videos. Concurrent ap-\nproaches [ 69,70] use a single vision transformer network to\njointly model the reference frames and the query frame with-\nout explicit memory reading operations. "}, {"title": "Section 0047", "content": "They attain high\naccuracy but require large-scale pretraining (e.g., MAE [ 71])\nand have a much lower inference speed ( <4frames per sec-\nond). Cutie is carefully designed to notcompute any (costly)\nattention between spatial feature maps in our object trans-\nformer while facilitating efficient global communication via\na small set of object queries \u2013 allowing Cutie to be real-time.\nObject-Level Reasoning. Early VOS algorithms [ 59,72,\n73] that attempt to reason at the object level use either re-\nidentification or k-means clustering to obtain object fea-\ntures and have a lower performance on standard benchmarks.\nHODOR [ 18], and its follow-up work TarViS [ 17], approach\nVOS with object-level descriptors which allow for greater\nflexibility (e.g., training on static images only [ 18] or ex-\ntending to different video segmentation tasks [ 17,74,75])\nbut fall short on VOS segmentation accuracy (e.g., [ 74]\nis 6.9 J&Fbehind state-of-the-art methods in DA VIS\n2017 [ 13]) due to under-using high-resolution features.\nISVOS [ 76] proposes to inject features from a pre-trained\ninstance segmentation network (i.e., Mask2Former [ 15]) into\na memory-based VOS method [ 51]. Cutie has a similar moti-\nvation but is crucially different in three ways: 1) Cutie learns\nobject-level information end-to-end, without needing to pre-\ntrain on instance segmentation tasks/datasets, 2) Cutie allows\nbi-directional communication between pixel-level features\nand object-level features for an integrated framework, and\n3) Cutie is a one-stage method that does not perform sepa-\nrate instance segmentation while ISVOS does \u2013 this allows\nCutie to run six times (estimated) faster. Moreover, ISVOS\ndoes not release code while we open source code for the\ncommunity which facilitates follow-up work.\nAutomatic Video Segmentation. "}, {"title": "Section 0058", "content": "Recently, video object\nsegmentation methods have been used as an integral com-\nponent in automatic video segmentation pipelines, such as\nopen-vocabulary/universal video segmentation (e.g., Track-\ning Anything [ 5,6], DEV A [ 7]) and unsupervised video\nsegmentation [ 77]. We believe the robustness and efficiency\nof Cutie are beneficial for these applications.\n2Query\nencoderMask\nencoder\nObject memory\n\ud835\udc46: \ud835\udc41\u00d7\ud835\udc36Pixel readout\n\ud835\udc450:\ud835\udc3b\u00d7\ud835\udc4a\u00d7\ud835\udc36\nDecoderObject queries\n\ud835\udc4b: \ud835\udc41\u00d7\ud835\udc36Object readout\n\ud835\udc45\ud835\udc3f:\ud835\udc3b\u00d7\ud835\udc4a\u00d7\ud835\udc36\n\ud835\udc4bPixel feature\nObject queries\nObject memory\u2295\n\ud835\udc46\ud835\udc450Masked cross \nattentionLinear\nMask \ud835\udc40\ud835\udc59\nQ\nK\nVSelf attention\nKQ\nVQuery FFNCross attentionQ\nK\nVPixel FFN\ud835\udc45\ud835\udc59\u22121\n\ud835\udc4b\ud835\udc59\u22121\ud835\udc43\ud835\udc45\ud835\udc43\ud835\udc4b\n\ud835\udc43\ud835\udc4b\ud835\udc43\ud835\udc45\n\ud835\udc43\ud835\udc4b\n\ud835\udc4b\ud835\udc59\ud835\udc45\ud835\udc59\ud835\udc59-th object transformer block\n\ud835\udc43\ud835\udc4b\nPixel positional embeddingQuery positional embedding\ud835\udc43\ud835\udc4b\n\ud835\udc43\ud835\udc45Object transformer\n\ud835\udc3f\u00d7 blocks\n\u2295Element -wise additionPixel memory\n\ud835\udc39:\ud835\udc47\u00d7\ud835\udc3b\u00d7\ud835\udc4a\u00d7\ud835\udc36\nMemory framesQuery frame\nFigure 2. Overview of Cutie. We store pixel memory Fand object memory Srepresentations from past segmented (memory) frames. Pixel\nmemory is retrieved for the query frame as pixel readout R0, which bidirectionally interacts with object queries Xand object memory Sin\nthe object transformer. The Lobject transformer blocks enrich the pixel feature with object-level semantics and produce the final RLobject\nreadout for decoding into the output mask. Standard residual connections, layer normalization, and skip-connections from the query encoder\nto the decoder are omitted for readability.\n3. Cutie\n3.1. Overview\nWe provide an overview of Cutie in Figure 2. For read-\nability, following prior works [ 8,9], we consider a single\ntarget object as the extension to multiple objects is straight-\nforward (see supplement). Following the standard semi-\nsupervised video object segmentation (VOS) setting, Cutie\ntakes a first-frame segmentation of target objects as input\nand segments subsequent frames sequentially in a stream-\ning fashion. "}, {"title": "Section 0070", "content": "First, Cutie encodes segmented frames (given\nas input or segmented by the model) into a high-resolution\npixel memory F(Section 3.4.1) and a high-level object mem-\noryS(Section 3.3) and stores them for segmenting future\nframes. To segment a new query frame, Cutie retrieves an\ninitial pixel readout R0from the pixel memory using en-\ncoded query features. This initial readout R0is computed\nvia low-level pixel matching and is therefore often noisy. We\nenrich it with object-level semantics by augmenting R0with\ninformation from the object memory Sand a set of object\nqueries Xthrough an object transformer withLtransformer\nblocks (Section 3.2). The enriched output of the object trans-\nformer, RL, or the object readout, is passed to the decoder\nfor generating the final output mask. In the following, we\nwill first describe the three main contributions of Cutie: ob-\nject transformer, masked attention, and object memory. Note,we derive the pixel memory from existing works [ 9], which\nwe only describe as implementation details in Section 3.4.1\nwithout claiming any contribution.\n3.2. Object Transformer\n3.2.1 Overview\nThe bottom of Figure 2 illustrates the object transformer. The\nobject transformer takes an initial readout R0\u2208RHW\u00d7C,\na set of Nend-to-end trained object queries X\u2208RN\u00d7C,\nand object memory S\u2208RN\u00d7Cas input, and integrates\nthem with Ltransformer blocks. Note HandWare image\ndimensions after encoding with stride 16. Before the first\nblock, we sum the static object queries with the dynamic\nobject memory for better adaptation, i.e., X0=X+S.\nEach transformer block bidirectionally allows the object\nqueries Xl\u22121to attend to the readout Rl\u22121, and vice versa,\nproducing updated queries Xland readout Rlas the output\nof the l-th block. The last block\u2019s readout, RL, is the final\noutput of the object transformer.\nWithin each block, we first compute masked cross-\nattention, letting the object queries Xl\u22121read from the\npixel features Rl\u22121. "}, {"title": "Section 0080", "content": "The masked attention focuses half\nof the object queries on the foreground region while the\nother half is targeted towards the background (details in Sec-\ntion 3.2.2). Then, we pass the object queries into standard\n3self-attention and feed-forward layers [ 66] for object-level\nreasoning. Next, we update the pixel features with a reversed\ncross-attention layer, putting the object semantics from ob-\nject queries Xlback into pixel features Rl\u22121. We then pass\nthe pixel features into a feed-forward network while skip-\nping the computationally expensive self-attention in a stan-\ndard transformer [ 66]. Throughout, positional embeddings\nare added to the queries and keys following [ 14,15] (Sec-\ntion 3.2.3). Residual connections and layer normalizations\nare used in every attention and feed-forward layer follow-\ning [ 78]. All attention layers are implemented with multi-\nhead scaled dot product attention [66]. Importantly,\n1.We carefully avoid any direct attention between high-\nresolution spatial features (e.g., R), as they are intensive\nin both memory and compute. Despite this, these spa-\ntial features can still interact globally via object queries,\nmaking each transformer block efficient and expressive.\n2.The object queries restructure the pixel features with\na residual contribution without discarding the high-\nresolution pixel features. This avoids irreversible dimen-\nsionality reductions (would be over 100 \u00d7) and keeps\nthose high-resolution features for accurate segmentation.\nNext, we describe the core components in our object trans-\nformer blocks: foreground/background masked attention and\nthe construction of the positional embeddings.\n3.2.2 Foreground-Background Masked Attention\nIn our (pixel-to-query) cross-attention, we aim to update\nthe object queries Xl\u2208RN\u00d7Cby attending over the pixel\nfeatures Rl\u2208RHW\u00d7C. "}, {"title": "Section 0088", "content": "Standard cross-attention with the\nresidual path finds\nX\u2032\nl=AlVl+Xl=softmax (QlKT\nl)Vl+Xl,(1)\nwhere Qlis a learned linear transformation of Xl, andKl, Vl\nare learned linear transformations of Rl. The rows of the\naffinity matrix Al\u2208RN\u00d7HWdescribe the attention of each\nobject query over the entire feature map. We note that there\nare distinctly different attention patterns for different object\nqueries \u2013 some focus on different foreground parts, some\non the background, and some on distractors (top of Fig-\nure 3). These object queries collect information from dif-\nferent regions of interest and integrate them in subsequent\nself-attention/feed-forward layers. However, the soft na-\nture of attention makes this process noisy and less reliable\n\u2013 queries that mainly attend to the foreground might have\nsmall weights distributed in the background and vice versa.\nInspired by [ 15], we deploy masked attention to aid the clean\nseparation of semantics between foreground and background.\nDifferent from [ 15], which only attends to the foreground,\nwe find it helpful to also attend to the background, especially\nin challenging tracking scenarios with distractors. In prac-\ntice, we let the first half of the object queries (i.e., foregroundqueries) always attend to the foreground and the second half\n(i.e., background queries) attend to the background. This\nmasking is shared across all attention heads.\nFormally, our foreground-background masked cross-\nattention finds\nX\u2032\nl=softmax (Ml+QlKT\nl)Vl+Xl, (2)\nwhere Ml\u2208 {0,\u2212\u221e}N\u00d7HWcontrols the attention mask-\ning \u2013 specifically, Ml(q, i)determines whether the q-th\nquery is allowed ( = 0) or not allowed ( =\u2212\u221e) to attend to\nthei-th pixel. To compute Ml, we first find a mask predic-\ntion at the current layer Ml, which is linearly projected from\nthe last pixel feature Rl\u22121and activated with the sigmoid\nfunction. "}, {"title": "Section 0098", "content": "Then, Mlis computed as\nMl(q, i) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f30, ifq\u2264N/2andMl(i)\u22650.5\n0, ifq > N/ 2andMl(i)<0.5\n\u2212\u221e,otherwise,(3)\nwhere the first case is for foreground attention and the sec-\nond is for background attention. Figure 3 (bottom) visu-\nalizes the attention maps after this foreground-background\nmasking. Note, despite the hard foreground-background sep-\naration, the object queries communicate in the subsequent\nself-attention layer for potential global feature interaction.\nNext, we discuss the positional embeddings used in object\nqueries and pixel features that allow location-based attention.\n3.2.3 Positional Embeddings\nSince vanilla attention operations are permutation equivari-\nant, positional embeddings are used to provide additional\nfeatures about the position of each token [ 66]. Following\nprior transformer-based vision networks [ 14,15], we add the\npositional embedding to the query and key features at every\nattention layer (Figure 2), and not to the value.\nFor the object queries, we use a positional embedding\nPX\u2208RN\u00d7Cthat combines an end-to-end learnable em-\nbedding EX\u2208RN\u00d7Cand the dynamic object memory\nS\u2208RN\u00d7Cvia\nPX=EX+fObjEmbed (S), (4)\nwhere fObjEmbed is a trainable linear projection.\nFor the pixel feature, the positional embedding PR\u2208\nRHW\u00d7Ccombines a fixed 2D sinusoidal positional embed-\ndingRsin[14] that encodes absolute pixel coordinates and\nthe initial readout R0\u2208RHW\u00d7Cvia\nPR=Rsin+fPixEmbed (R0), (5)\nwhere fPixEmbed is another trainable linear projection. Note\nthat the sinusoidal embedding Rsinoperates on normalized\ncoordinates and is scaled accordingly to different image sizes\nat test time.\n4Figure 3. Visualization of cross-attention weights (rows of AL) in the object transformer. The middle cat is the target object. Top:\nwithout foreground-background masking \u2013 some queries mix semantics from foreground and background (framed in red). Bottom: with\nforeground-background masking. The leftmost three are foreground queries, and the rightmost three are background queries. "}, {"title": "Section 0110", "content": "Semantics is\nthus cleanly separated. The f.g./b.g. queries can communicate in the subsequent self-attention layer. Note the queries attend to different\nforeground regions, distractors, and background regions.\n3.3. Object Memory\nIn the object memory S\u2208RN\u00d7C, we store a compact set\nofNvectors which make up a high-level summary of the\ntarget object. This object memory is used in the object\ntransformer (Section 3.2) to provide target-specific features.\nAt a high level, we compute Sby mask-pooling over all\nencoded object features with Ndifferent masks. Concretely,\ngiven object features U\u2208RTHW\u00d7CandNpooling masks\n{Wq\u2208[0,1]THW,0< q\u2264N}, where Tis the number\nof memory frames, the q-th object memory Sq\u2208RCis\ncomputed by\nSq=PTHW\ni=1U(i)Wq(i)\nPTHW\ni=1Wq(i). (6)\nDuring inference, we use a classic streaming average algo-\nrithm such that this operation takes constant time and mem-\nory with respect to the video length. See the supplement\nfor details. Note, an object memory vector Sqwould not\nbe modified if the corresponding pooling weights are zero,\ni.e.,PHW\ni=1Wt\nq(i) = 0 , preventing feature drifting when the\ncorresponding object region is not visible (e.g., occluded).\nTo find UandWfor a memory frame, we first encode\nthe corresponding image Iand the segmentation mask M\nwith the mask encoder for memory feature F\u2208RTHW\u00d7C.\nWe use a 2-layer, C-dimensional MLP fObjFeat to obtain the\nobject feature Uvia\nU=fObjFeat (F). (7)\nFor the Npooling masks {Wq\u2208[0,1]THW,0< q\u2264N},\nwe additionally apply foreground-background separation\nas detailed in Section 3.2.2 and augment it with a fixed\n2D sinusoidal positional embedding Rsin(as mentioned in\nSection 3.2.3). The separation allows it to aggregate clean\nsemantics during pooling, while the positional embedding\nenables location-aware pooling. "}, {"title": "Section 0120", "content": "Formally, we compute thei-th pixel of the q-th pooling mask via\nWq(i) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f30,ifq\u2264N/2andM(i)<0.5\n0,ifq > N/ 2andM(i)\u22650.5\n\u03c3(fPoolWeight (F(i) +Rsin(i))),otherwise,\n(8)\nwhere \u03c3is the sigmoid function, fPoolWeight is a 2-layer, N-\ndimensional MLP, and the segmentation mask Mis down-\nsampled to match the feature stride of F.\n3.4. Implementation Details\n3.4.1 Pixel Memory\nOur pixel memory design, which provides the pixel feature\nR0(see Figure 2), is derived from XMem [ 5,9] working and\nsensory memory. We do not claim contributions. Here, we\npresent the high-level algorithm and defer details to the sup-\nplementary material. The pixel memory is composed of an\nattentional component (with keys k\u2208RTHW\u00d7Ckand val-\nuesv\u2208RTHW\u00d7C) and a recurrent component (with hidden\nstate hHW\u00d7C). Long-term memory [ 9] can be optionally\nincluded in the attentional component without re-training\nfor better performance on long videos. The keys and val-\nues consist of low-level appearance features for matching\nwhile the hidden state provides temporally consistent fea-\ntures. To retrieve a pixel readout R0, we first encode the\nquery frame to obtain query feature qHW\u00d7C, and compute\nthe query-to-memory affinity Apix\u2208[0,1]HW\u00d7THWvia\nApix\nij=exp (d(qi,kj))P\nmexp (d(qi,km)), (9)\nwhere d(\u00b7,\u00b7)is the anisotropic L2 function [ 9] which is pro-\nportional to the similarity between the two inputs. Finally,\nwe find the pixel readout R0by combining the attention\nreadout with the hidden state:\nR0=ffuse\u0000\nApixv+h\u0001\n, (10)\n5where ffuseis a small network consisting of two C-dimension\nconvolutional residual blocks with channel attention [79].\n3.4.2 Network Architecture\nWe study two model variants: \u2018small\u2019 and \u2018base\u2019 with differ-\nent query encoder backbones, otherwise sharing the same\nconfiguration: C= 256 channels with L= 3object trans-\nformer blocks and N= 16 object queries.\nConvNets. We parameterize the query encoder and the\nmask encoder with ResNets [ 80]. "}, {"title": "Section 0136", "content": "Following [ 8,9], we dis-\ncard the last convolutional stage and use the stride 16 feature.\nFor the query encoder, we use ResNet-18 for the small model\nand ResNet-50 for the base model. For the mask encoder, we\nuse ResNet-18. \u2018Cutie-base\u2019 thus shares the same backbone\nconfiguration as XMem. We find that Cutie works well with\na lighter decoder \u2013 we use a similar iterative upsampling\narchitecture as in XMem but halve the number of channels\nin all upsampling blocks for better efficiency.\nFeed-Forward Networks (FFN). We use query FFN and\npixel FFN in our object transformer block (Figure 2). For\nthe query FFN, we use a 2-layer MLP with a hidden size\nof8C= 2048 . For the pixel FFN, we use two 3\u00d73con-\nvolutions with a smaller hidden size of C= 256 to reduce\ncomputation. As we do not use self-attention on the pixel\nfeatures, we compensate by using efficient channel atten-\ntion [ 79] after the second convolution of the pixel FFN. Layer\nnormalizations are applied to the query FFN following [ 78]\nand not to the pixel FFN, as we observe no empirical benefits.\nReLU is used as the activation function.\n3.4.3 Training\nData. Following [ 8\u201310], we first pretrain our network on\nstatic images [ 81\u201385] by generating three-frame sequences\nwith synthetic deformation. Next, we perform the main train-\ning on video datasets DA VIS [ 13] and YouTubeVOS [ 19] by\nsampling eight frames following [ 9]. We optionally also train\non MOSE [ 12] (combined with DA VIS and YouTubeVOS),\nas we notice the training sets of YouTubeVOS and DA VIS\nhave become too easy for our model to learn from (>93%\nIoU during training). For every setting, we use one trained\nmodel and do not finetune for specific datasets. We addi-\ntionally introduce a \u2018MEGA\u2019 setting with BURST [ 3] and\nOVIS [ 86] included in training (+1.6 J&Fin MOSE). De-\ntails are provided in the supplementary material.\nOptimization. We use the AdamW [ 87] optimizer with a\nlearning rate of 1e\u22124, a batch size of 16, and a weight decay\nof 0.001. "}, {"title": "Section 0149", "content": "Pretraining lasts for 80K iterations with no learning\nrate decay. Main training lasts for 125K iterations, with the\nlearning rate reduced by 10 times after 100K and 115K\niterations. The query encoder has a learning rate multiplier\nof 0.1 following [ 5,10,15] to mitigate overfitting. Followingthe bag of tricks from DEV A [ 5], we clip the global gradient\nnorm to 3 throughout and use stable data augmentation. The\nentire training process takes approximately 30 hours on four\nA100 GPUs for the small model.\nLosses. Following [ 15], we adopt point supervision which\ncomputes the loss only at Ksampled points instead of the\nwhole mask. We use importance sampling [ 88] and set\nK= 8192 during pretraining and K= 12544 during main\ntraining. We use a combined loss function of cross-entropy\nand soft dice loss with equal weighting following [5, 9, 10].\nIn addition to the loss applied to the final segmentation out-\nput, we adopt auxiliary losses in the same form (scaled by\n0.01) to the intermediate masks Mlin the object transformer.\n3.4.4 Inference\nDuring testing, we encode a memory frame for updating the\npixel memory and the object memory every r-th frame. r\ndefaults to 5 following [ 9]. For the keys kand values vin the\nattention component of the pixel memory, we always keep\nfeatures from the first frame (as it is given by the user) and\nuse a First-In-First-Out (FIFO) approach for other memory\nframes to ensure the total number of memory frames Tis\nless than or equal to a pre-defined limit Tmax= 5. For\nprocessing long videos (e.g., BURST [ 3] or LVOS [ 89] with\nover a thousand frames per video), we use the long-term\nmemory [ 9] instead of FIFO without re-training, following\nthe default parameters in [ 9]. For the pixel memory, we use\ntop-kfiltering [ 2] with k= 30 . Inference is fully online,\ncan be streamed, and uses a constant amount of compute per\nframe and memory with respect to the sequence length.\n4. "}, {"title": "Section 0156", "content": "Experiments\nFor evaluation, we use standard metrics: Jaccard index\nJ, contour accuracy F, and their average J&F[13]. In\nYouTubeVOS [ 19],JandFare computed for \u201cseen\u201d and\n\u201cunseen\u201d categories separately. Gis the averaged J&Ffor\nboth seen and unseen classes. For BURST [ 3], we assess\nHigher Order Tracking Accuracy (HOTA) [ 90] on common\nand uncommon object classes separately. For our models,\nunless otherwise specified, we resize the inputs such that the\nshorter edge has no more than 480 pixels and rescale the\nmodel\u2019s prediction back to the original resolution.\n4.1. Main Results\nWe compare with several state-of-the-art approaches on\nrecent standard benchmarks: DA VIS 2017 validation/test-\ndev [ 13] and YouTubeVOS validation [ 19]. To assess the\nrobustness of VOS algorithms, we also report results on\nMOSE validation [ 12], which contains heavy occlusions and\ncrowded environments for evaluation. "}, {"title": "Section 0161", "content": "DA VIS 2017 [ 13] con-\ntains annotated videos at 24 frames per second (fps), while\nYouTubeVOS contains videos at 30fps but is only annotated\n6MOSE DA VIS-17 val DA VIS-17 test YouTubeVOS-2019 val\nMethod J&FJFJ&FJFJ&FJF GJsFsJuFuFPS\nTrained without MOSE\nSTCN [51] 52.5 48.5 56.6 85.4 82.2 88.6 76.1 72.7 79.6 82.7 81.1 85.4 78.2 85.9 13.2\nAOT-R50 [10] 58.4 54.3 62.6 84.9 82.3 87.5 79.6 75.9 83.3 85.3 83.9 88.8 79.9 88.5 6.4\nRDE [55] 46.8 42.4 51.3 84.2 80.8 87.5 77.4 73.6 81.2 81.9 81.1 85.5 76.2 84.8 24.4\nXMem [9] 56.3 52.1 60.6 86.2 82.9 89.5 81.0 77.4 84.5 85.5 84.3 88.6 80.3 88.6 22.6\nDeAOT-R50 [68] 59.0 54.6 63.4 85.2 82.2 88.2 80.7 76.9 84.5 85.6 84.2 89.2 80.2 88.8 11.7\nSimVOS-B [69] - - - 81.3 78.8 83.8 - - - - - - - - 3.3\nJointFormer [70] - - - - - - 65.6 61.7 69.4 73.3 75.2 78.5 65.8 73.6 3.0\nISVOS [76] - - - 80.0 76.9 83.1 - - - - - - - -5.8\u2217\nDEV A [5] 60.0 55.8 64.3 86.8 83.6 90.0 82.3 78.7 85.9 85.5 85.0 89.4 79.7 88.0 25.3\nCutie-small 62.2 58.2 66.2 87.2 84.3 90.1 84.1 80.5 87.6 86.2 85.3 89.6 80.9 89.0 45.5\nCutie-base 64.0 60.0 67.9 88.8 85.4 92.3 84.2 80.6 87.7 86.1 85.5 90.0 80.6 88.3 36.4\nTrained with MOSE\nXMem [9] 59.6 55.4 63.7 86.0 82.8 89.2 79.6 76.1 83.0 85.6 84.1 88.5 81.0 88.9 22.6\nDeAOT-R50 [68] 64.1 59.5 68.7 86.0 83.1 88.9 82.8 79.1 86.5 85.3 84.2 89.0 79.9 88.2 11.7\nDEV A [5] 66.0 61.8 70.3 87.0 83.8 90.2 82.6 78.9 86.4 85.4 84.9 89.4 79.6 87.8 25.3\nCutie-small 67.4 63.1 71.7 86.5 83.5 89.5 83.8 80.2 87.5 86.3 85.2 89.7 81.1 89.2 45.5\nCutie-base 68.3 64.2 72.3 88.8 85.6 91.9 85.3 81.4 89.3 86.5 85.4 90.0 81.3 89.3 36.4\nTable 1. Quantitative comparison on video object segmentation benchmarks. All algorithms with available code are re-run on our hardware\nfor a fair comparison. We could not obtain the code for [ 69,70,76] at the time of writing, and thus they cannot be reproduced on datasets\nthat they do not report results on. For a fair comparison, all methods in this table use ImageNet [ 91] pre-training only or are trained from\nscratch. "}, {"title": "Section 0182", "content": "We compare methods with external pre-training (e.g., MAE [71] pre-training) in the supplement.\u2217estimated FPS.\nBURST val BURST test\nMethod AllCom. Unc. AllCom. Unc. Mem.\nDeAOT [68] FIFO 51.3 56.3 50.0 53.2 53.5 53.2 10.8G\nDeAOT [68] INF 56.4 59.7 55.5 57.9 56.7 58.1 34.9G\nXMem [9] FIFO 52.9 56.0 52.1 55.9 57.6 55.6 3.03G\nXMem [9] LT 55.1 57.9 54.4 58.2 59.5 58.0 3.34G\nCutie-small FIFO 56.8 61.1 55.8 61.1 62.4 60.8 1.35G\nCutie-small LT 58.3 61.5 57.5 61.6 63.1 61.3 2.28G\nCutie-base LT 58.4 61.8 57.5 62.6 63.8 62.3 2.36G\nTable 2. Comparisons of performance on long videos on the\nBURST dataset [ 3]. Mem.: maximum GPU memory usage. FIFO:\nfirst-in-first-out memory bank; INF: unbounded memory; LT: long-\nterm memory [ 9]. DeAOT [ 68] is not compatible with long-term\nmemory. All methods are trained with the MOSE [12] dataset.\nat 6fps. For a fair comparison, we evaluate all algorithms at\nfull fps whenever possible, which is crucial for video editing\nand for having a smooth user-interaction experience. For this,\nwe re-run (De)AOT [ 10,68] with their official code at 30fps\non YouTubeVOS. We also retrain XMem [ 9], DeAOT [ 68],\nand DEV A [ 5] with their official code to include MOSE as\ntraining data (in addition to YouTubeVOS and DA VIS). For\nlong video evaluation, we test on BURST [ 3] and LVOS [ 89]\nand experiment with the long-term memory [ 9] in addition\nto our default FIFO memory strategy. See supplement for\ndetails. We compare with DeAOT [ 68] and XMem [ 9] underthe same setting.\nTable 1 and Table 2 list our findings. Our method is\nhighlighted with lavender . FPS is recorded on YouTubeVOS\nwith a V100. Results on YouTubeVOS-18 and LVOS [ 89]\nare provided in the supplement. Cutie achieves better results\nthan state-of-the-art methods, especially on the challenging\nMOSE dataset, while remaining efficient.\n4.2. Ablations\nHere, we study various design choices of our algorithm. "}, {"title": "Section 0203", "content": "We\nuse the small model variant with MOSE [ 12] training data.\nWe highlight our default configuration with lavender . For\nablations, we report the J&Ffor MOSE validation and FPS\non YouTubeVOS-2019 validation when applicable. Due to\nresource constraints, we train a selected subset of ablations\nthree times with different random seeds and report mean \u00b1std.\nThe baseline is trained five times. In tables that do not report\nstd, we present our performance with the default random\nseed only.\nHyperparameter Choices. Table 3 compares our results\nwith different choices of hyperparameters: number of object\ntransformer blocks L, number of object queries N, interval\nbetween memory frames r, and maximum number of mem-\nory frames Tmax. Note that L= 0is equivalent to not having\nan object transformer. We visualize the progression of pixel\nfeatures in Figure 4. We find that the object transformer\n7Setting J&F FPS\nNumber of transformer blocks\nL= 0 65.2 56.6\nL= 1 66.0 51.1\nL= 3 67.4 45.5\nL= 5 67.8 37.1\nNumber of object queries\nN= 8 67.6 45.5\nN= 16 67.4 45.5\nN= 32 67.2 45.5\nMemory interval\nr= 3 68.9 43.2\nr= 5 67.4 45.5\nr= 7 67.0 46.4\nMax. memory frames\nTmax= 3 66.9 48.5\nTmax= 5 67.4 45.5\nTmax= 10 67.6 37.4\nTable 3. Performance comparison\nwith different choices of hyperpa-\nrameters.Setting J&F FPS\nBoth 67.3\u00b10.36 45.5\nBottom-up only 65.0 \u00b10.44 56.6\nTop-down only 40.7 \u00b11.62 46.9\nTable 4. Comparison of our approach with\nbottom-up-only (no object transformer) and\ntop-down-only (no pixel memory).Setting J&F\nWith both 67.3\u00b10.36\nNo object memory ( X) 66.9 \u00b10.26\nNo object query ( S) 67.2\u00b10.10\nTable 5. Ablations on the dynamic object memory\nand the static object query. Running times are\nsimilar.\nSetting J&F FPS\nf.g.-b.g. masked attn. 67.3\u00b10.36 45.5\nf.g. masked attn. only 66.7 \u00b10.21 45.5\nNo masked attn. 63.8 \u00b11.06 46.3\nTable 6. Ablations on foreground-background masked\nattention and object memory.Setting J&F\nWith both p.e. 67.4\nWithout query p.e. 66.5\nWithout pixel p.e. "}, {"title": "Section 0218", "content": "66.2\nWith neither 66.1\nTable 7. Ablations on positional embed-\ndings. Running times are similar.\nImage M1 M2 M3 Final mask\nFigure 4. Visualization of auxiliary masks ( Ml) at different layers of the object transformer. At\nevery layer, noises are suppressed (pink arrows) and the target object becomes more coherent\n(yellow arrows).\nblocks effectively suppress noises from distractors and pro-\nduce more coherent object masks. Cutie is insensitive to the\nnumber of object queries \u2013 we think this is because 8 queries\nare sufficient to model the foreground/background of a single\ntarget object. As these queries execute in parallel, we find no\nnoticeable differences in running time. Cutie benefits from\nhaving a shorter memory interval and a larger memory bank\nat the cost of a slower running time (e.g., +2.2 J&Fon\nMOSE with half the speed) \u2013 we explore this speed-accuracy\ntrade-off (as Cutie+) without re-training in the supplement.\nBottom-Up v.s. Top-Down Feature. Table 4 reports our\nfindings. We compare a bottom-up-only approach (similar to\nXMem [ 9] with the training tricks [ 5] and a lighter backbone)\nwithout the object transformer, a top-down-only approach\nwithout the pixel memory, and our approach with both. Ours,\nintegrating both features, performs the best.\nMasked Attention Table 6 shows our results with different\nmasked attention configurations. Masked attention is crucial\nfor good performance \u2013 we hypothesize that using full at-\ntention produces confusing signals (especially in cluttered\nsettings, see supplement), which leads to poor generalization.\nWe note that using full attention also leads to rather unstable\ntraining. We experimented with different distributions of\nf.g./b.g. queries with no significant observed effects.\nObject Memory and Positional Embeddings. "}, {"title": "Section 0234", "content": "Table 5 and\nTable 7 ablate on the object memory ( X), the object query\n(S), and the positional embeddings in the object transformer.We note that the object query, while standard, is not useful\nfor Cutie in the presence of the object memory. Positional\nembeddings are commonly used and do help.\n4.3. Limitations\nDespite being more robust, Cutie often fails when highly\nsimilar objects move in close proximity or occlude each other.\nThis problem is not unique to Cutie. We suspect that, in these\ncases, neither the pixel memory nor the object memory is\nable to pick up sufficiently discriminative features for the\nobject transformer to operate on. We provide visualizations\nin the supplementary material.\n5. Conclusion\nWe present Cutie, an end-to-end network with object-level\nmemory reading for robust video object segmentation in\nchallenging scenarios. Cutie efficiently integrates top-down\nand bottom-up features, achieving new state-of-the-art re-\nsults in several benchmarks, especially on the challenging\nMOSE dataset. We hope to draw more attention to object-\ncentric video segmentation and to enable more accessible\nuniversal video segmentation methods via integration with\nsegment-anything models [4, 5].\nAcknowledgments . Work supported in part by NSF grants\n2008387, 2045586, 2106825, MRI 1725729 (HAL [ 92]), and NIFA\naward 2020-67021-32799.\n8References\n[1]Vladim\u00edr Petr\u00edk, Mohammad Nomaan Qureshi, Josef Sivic,\nand Makar Tapaswi. Learning object manipulation skills from\nvideo via approximate differentiable physics. In IROS , 2022.\n1\n[2]Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Modular\ninteractive video object segmentation: Interaction-to-mask,\npropagation and difference-aware fusion. In CVPR , 2021. 1,\n6, 16, 19, 21\n[3]Ali Athar, Jonathon Luiten, Paul V oigtlaender, Tarasha Khu-\nrana, Achal Dave, Bastian Leibe, and Deva Ramanan. Burst:\nA benchmark for unifying object recognition, segmentation\nand tracking in video. In WACV , 2023. "}, {"title": "Section 0265", "content": "1, 6, 7, 14, 15, 16, 17,\n18\n[4]Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In arXiv , 2023. 1, 8\n[5]Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander\nSchwing, and Joon-Young Lee. Tracking anything with de-\ncoupled video segmentation. In ICCV , 2023. 1, 2, 5, 6, 7, 8,\n16, 18, 20\n[6]Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang,\nand Feng Zheng. Track anything: Segment anything meets\nvideos. In arXiv , 2023. 2\n[7]Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin\nYang, Wenguan Wang, and Yi Yang. Segment and track\nanything. In arXiv , 2023. 1, 2\n[8]Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo\nKim. Video object segmentation using space-time memory\nnetworks. In ICCV , 2019. 1, 2, 3, 6, 18, 19, 20\n[9]Ho Kei Cheng and Alexander G Schwing. XMem: Long-term\nvideo object segmentation with an atkinson-shiffrin memory\nmodel. In ECCV , 2022. 1, 2, 3, 5, 6, 7, 8, 12, 14, 15, 16, 17,\n18, 19, 20, 21\n[10] Zongxin Yang, Yunchao Wei, and Yi Yang. Associating\nobjects with transformers for video object segmentation. In\nNeurIPS , 2021. 1, 2, 6, 7, 19\n[11] Maksym Bekuzarov, Ariana Bermudez, Joon-Young Lee, and\nHao Li. Xmem++: Production-level video segmentation from\nfew annotated frames. In ICCV , 2023. 1, 2, 21\n[12] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang,\nPhilip HS Torr, and Song Bai. MOSE: A new dataset for\nvideo object segmentation in complex scenes. In arXiv , 2023.\n1, 2, 6, 7, 15, 16, 17, 18, 20\n[13] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc\nVan Gool, Markus Gross, and Alexander Sorkine-Hornung.\nA benchmark dataset and evaluation methodology for video\nobject segmentation. In CVPR , 2016. 1, 2, 6, 15, 16, 17, 18,\n20\n[14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV , 2020. "}, {"title": "Section 0290", "content": "1,\n4\n[15] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander\nKirillov, and Rohit Girdhar. Masked-attention mask trans-former for universal image segmentation. In CVPR , 2022. 2,\n4, 6, 15, 16, 20\n[16] Junfeng Wu, Qihao Liu, Yi Jiang, Song Bai, Alan Yuille, and\nXiang Bai. In defense of online models for video instance\nsegmentation. In ECCV , 2022.\n[17] Ali Athar, Alexander Hermans, Jonathon Luiten, Deva\nRamanan, and Bastian Leibe. Tarvis: A unified ap-\nproach for target-based video segmentation. arXiv preprint\narXiv:2301.02657 , 2023. 2\n[18] Ali Athar, Jonathon Luiten, Alexander Hermans, Deva Ra-\nmanan, and Bastian Leibe. Hodor: High-level object descrip-\ntors for object re-segmentation in video learned from static\nimages. In CVPR , 2022. 1, 2\n[19] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen\nLiang, Jianchao Yang, and Thomas Huang. Youtube-vos: A\nlarge-scale video object segmentation benchmark. In ECCV ,\n2018. 2, 6, 12, 15, 16, 17, 18, 20\n[20] Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura\nLeal-Taix\u00e9, Daniel Cremers, and Luc Van Gool. One-shot\nvideo object segmentation. In CVPR , 2017. 2\n[21] Paul V oigtlaender and Bastian Leibe. Online adaptation of\nconvolutional neural networks for video object segmentation.\nInBMVC , 2017.\n[22] K-K Maninis, Sergi Caelles, Yuhua Chen, Jordi Pont-Tuset,\nLaura Leal-Taix\u00e9, Daniel Cremers, and Luc Van Gool. Video\nobject segmentation without temporal information. In PAMI ,\n2018.\n[23] Goutam Bhat, Felix J\u00e4remo Lawin, Martin Danelljan, An-\ndreas Robinson, Michael Felsberg, Luc Van Gool, and Radu\nTimofte. Learning what to learn for video object segmentation.\nInECCV , 2020.\n[24] Andreas Robinson, Felix Jaremo Lawin, Martin Danelljan,\nFahad Shahbaz Khan, and Michael Felsberg. Learning fast\nand robust target models for video object segmentation. In\nCVPR , 2020. 2\n[25] Federico Perazzi, Anna Khoreva, Rodrigo Benenson, Bernt\nSchiele, and Alexander Sorkine-Hornung. "}, {"title": "Section 0316", "content": "Learning video\nobject segmentation from static images. In CVPR , 2017. 2\n[26] Yuan-Ting Hu, Jia-Bin Huang, and Alexander Schwing.\nMaskrnn: Instance level video object segmentation. In NIPS ,\n2017.\n[27] Ping Hu, Gang Wang, Xiangfei Kong, Jason Kuen, and Yap-\nPeng Tan. Motion-guided cascaded refinement network for\nvideo object segmentation. In CVPR , 2018.\n[28] Seoung Wug Oh, Joon-Young Lee, Kalyan Sunkavalli, and\nSeon Joo Kim. Fast video object segmentation by reference-\nguided mask propagation. In CVPR , 2018.\n[29] Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and\nPhilip HS Torr. Fast online object tracking and segmentation:\nA unifying approach. In CVPR , 2019.\n[30] Lu Zhang, Zhe Lin, Jianming Zhang, Huchuan Lu, and You\nHe. Fast video object segmentation via dynamic targeting\nnetwork. In ICCV , 2019.\n[31] Carles Ventura, Miriam Bellver, Andreu Girbau, Amaia Sal-\nvador, Ferran Marques, and Xavier Giro-i Nieto. Rvos: End-\nto-end recurrent network for video object segmentation. In\nCVPR , 2019. 2\n9[32] Yuan-Ting Hu, Jia-Bin Huang, and Alexander G Schwing.\nVideomatch: Matching based video object segmentation. In\nECCV , 2018. 2\n[33] Paul V oigtlaender, Yuning Chai, Florian Schroff, Hartwig\nAdam, Bastian Leibe, and Liang-Chieh Chen. Feelvos: Fast\nend-to-end embedding learning for video object segmentation.\nInCVPR , 2019.\n[34] Ziqin Wang, Jun Xu, Li Liu, Fan Zhu, and Ling Shao. Ranet:\nRanking attention network for fast video object segmentation.\nInICCV , 2019.\n[35] Kevin Duarte, Yogesh S. Rawat, and Mubarak Shah. Cap-\nsulevos: Semi-supervised video object segmentation using\ncapsule routing. In ICCV , 2019.\n[36] Zongxin Yang, Yunchao Wei, and Yi Yang. Collaborative\nvideo object segmentation by foreground-background integra-\ntion. In ECCV , 2020. 2\n[37] Yu Li, Zhuoran Shen, and Ying Shan. Fast video object\nsegmentation using the global context module. "}, {"title": "Section 0338", "content": "In ECCV ,\n2020.\n[38] Yizhuo Zhang, Zhirong Wu, Houwen Peng, and Stephen Lin.\nA transductive approach for video object segmentation. In\nCVPR , 2020.\n[39] Hongje Seong, Junhyuk Hyun, and Euntai Kim. Kernelized\nmemory network for video object segmentation. In ECCV ,\n2020.\n[40] Xiankai Lu, Wenguan Wang, Danelljan Martin, Tianfei Zhou,\nJianbing Shen, and Van Gool Luc. Video object segmentation\nwith episodic graph memory networks. In ECCV , 2020.\n[41] Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video\nobject segmentation with adaptive feature bank and uncertain-\nregion refinement. In NeurIPS , 2020.\n[42] Xuhua Huang, Jiarui Xu, Yu-Wing Tai, and Chi-Keung Tang.\nFast video object segmentation with temporal aggregation\nnetwork and dynamic template matching. In CVPR , 2020.\n[43] Shuxian Liang, Xu Shen, Jianqiang Huang, and Xian-Sheng\nHua. Video object segmentation with dynamic memory net-\nworks and adaptive object alignment. In ICCV , 2021.\n[44] Xiaohao Xu, Jinglu Wang, Xiao Li, and Yan Lu. Reliable\npropagation-correction modulation for video object segmen-\ntation. In AAAI , 2022.\n[45] Wenbin Ge, Xiankai Lu, and Jianbing Shen. Video object\nsegmentation using global and instance embedding learning.\nInCVPR , 2021.\n[46] Li Hu, Peng Zhang, Bang Zhang, Pan Pan, Yinghui Xu,\nand Rong Jin. Learning position and target consistency for\nmemory-based video object segmentation. In CVPR , 2021.\n[47] Haochen Wang, Xiaolong Jiang, Haibing Ren, Yao Hu, and\nSong Bai. Swiftnet: Real-time video object segmentation. In\nCVPR , 2021.\n[48] Haozhe Xie, Hongxun Yao, Shangchen Zhou, Shengping\nZhang, and Wenxiu Sun. Efficient regional memory network\nfor video object segmentation. In CVPR , 2021.\n[49] Hongje Seong, Seoung Wug Oh, Joon-Young Lee, Seongwon\nLee, Suhyeon Lee, and Euntai Kim. Hierarchical memory\nmatching network for video object segmentation. In ICCV ,\n2021.[50] Yunyao Mao, Ning Wang, Wengang Zhou, and Houqiang\nLi. "}, {"title": "Section 0366", "content": "Joint inductive and transductive learning for video object\nsegmentation. In ICCV , 2021. 2\n[51] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Rethink-\ning space-time networks with improved memory coverage for\nefficient video object segmentation. In NeurIPS , 2021. 2, 7,\n18, 19, 20\n[52] Yong Liu, Ran Yu, Fei Yin, Xinyuan Zhao, Wei Zhao, Wei-\nhao Xia, and Yujiu Yang. Learning quality-aware dynamic\nmemory for video object segmentation. In ECCV , 2022.\n[53] Ye Yu, Jialin Yuan, Gaurav Mittal, Li Fuxin, and Mei Chen.\nBatman: Bilateral attention transformer in motion-appearance\nneighboring space for video object segmentation. In ECCV ,\n2022. 2\n[54] Bo Miao, Mohammed Bennamoun, Yongsheng Gao, and\nAjmal Mian. Region aware video object segmentation with\ndeep motion modeling. In arXiv , 2022.\n[55] Mingxing Li, Li Hu, Zhiwei Xiong, Bang Zhang, Pan Pan,\nand Dong Liu. Recurrent dynamic embedding for video object\nsegmentation. In CVPR , 2022. 7\n[56] Kwanyong Park, Sanghyun Woo, Seoung Wug Oh, In So\nKweon, and Joon-Young Lee. Per-clip video object segmen-\ntation. In CVPR , 2022.\n[57] Yong Liu, Ran Yu, Jiahao Wang, Xinyuan Zhao, Yitong Wang,\nYansong Tang, and Yujiu Yang. Global spectral filter memory\nnetwork for video object segmentation. In ECCV , 2022.\n[58] Yurong Zhang, Liulei Li, Wenguan Wang, Rong Xie, Li Song,\nand Wenjun Zhang. Boosting video object segmentation via\nspace-time correspondence learning. In CVPR , 2023.\n[59] Xiaohao Xu, Jinglu Wang, Xiang Ming, and Yan Lu. To-\nwards robust video object segmentation with adaptive object\ncalibration. In ACM MM , 2022. 2\n[60] Suhwan Cho, Heansung Lee, Minjung Kim, Sungjun Jang,\nand Sangyoun Lee. Pixel-level bijective matching for video\nobject segmentation. In WACV , 2022.\n[61] Kai Xu and Angela Yao. Accelerating video object segmenta-\ntion with compressed video. In CVPR , 2022.\n[62] Roy Miles, Mehmet Kerim Yucel, Bruno Manganelli, and\nAlbert Saa-Garriga. "}, {"title": "Section 0396", "content": "Mobilevos: Real-time video object seg-\nmentation contrastive learning meets knowledge distillation.\nInCVPR , 2023.\n[63] Kun Yan, Xiao Li, Fangyun Wei, Jinglu Wang, Chenbin\nZhang, Ping Wang, and Yan Lu. Two-shot video object seg-\nmentation. In CVPR , 2023.\n[64] Rui Sun, Yuan Wang, Huayu Mai, Tianzhu Zhang, and Feng\nWu. Alignment before aggregation: trajectory memory re-\ntrieval network for video object segmentation. In ICCV , 2023.\n2\n[65] Zongxin Yang, Yunchao Wei, and Yi Yang. Collabora-\ntive video object segmentation by multi-scale foreground-\nbackground integration. In TPAMI , 2021. 2\n[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS , 2017. 2, 4\n[67] Brendan Duke, Abdalla Ahmed, Christian Wolf, Parham\nAarabi, and Graham W Taylor. Sstvos: Sparse spatiotem-\nporal transformers for video object segmentation. In CVPR ,\n2021. 2\n10[68] Zongxin Yang and Yi Yang. Decoupling features in hierarchi-\ncal propagation for video object segmentation. In NeurIPS ,\n2022. 2, 7, 12, 17, 19, 20\n[69] Qiangqiang Wu, Tianyu Yang, Wei Wu, and Antoni Chan.\nScalable video object segmentation with simplified frame-\nwork. In ICCV , 2023. 2, 7, 16\n[70] Jiaming Zhang, Yutao Cui, Gangshan Wu, and Limin Wang.\nJoint modeling of feature, correspondence, and a compressed\nmemory for video object segmentation. In arXiv , 2023. 2, 7,\n15, 16\n[71] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u2019ar, and Ross B Girshick. Masked autoencoders are\nscalable vision learners. 2022 ieee. In CVPR , 2021. 2, 7, 16\n[72] Xiaoxiao Li and Chen Change Loy. Video object segmen-\ntation with joint re-identification and attention-aware mask\npropagation. In ECCV , 2018. 2\n[73] Jonathon Luiten, Paul V oigtlaender, and Bastian Leibe. Pre-\nmvos: Proposal-generation, refinement and merging for video\nobject segmentation. In ACCV , 2018. "}, {"title": "Section 0424", "content": "2\n[74] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Ze-\nhuan Yuan, and Huchuan Lu. Universal instance perception\nas object discovery and retrieval. In CVPR , 2023. 2\n[75] Bin Yan, Yi Jiang, Peize Sun, Dong Wang, Zehuan Yuan,\nPing Luo, and Huchuan Lu. Towards grand unification of\nobject tracking. In ECCV , 2022. 2\n[76] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo,\nChuanxin Tang, Xiyang Dai, Yucheng Zhao, Yujia Xie,\nLu Yuan, and Yu-Gang Jiang. Look before you match: In-\nstance understanding matters in video object segmentation.\nInCVPR , 2023. 2, 7, 15, 16\n[77] Shubhika Garg and Vidit Goel. Mask selection and propaga-\ntion for unsupervised video object segmentation. In WCAV ,\n2021. 2\n[78] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin\nZheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei\nWang, and Tieyan Liu. On layer normalization in the trans-\nformer architecture. In ICLR , 2020. 4, 6\n[79] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wang-\nmeng Zuo, and Qinghua Hu. Eca-net: efficient channel atten-\ntion for deep convolutional neural networks. In CVPR , 2020.\n6\n[80] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR , 2016.\n6, 20\n[81] Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia. Hierarchical\nimage saliency detection on extended cssd. In TPAMI , 2015.\n6, 15, 19\n[82] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng,\nDong Wang, Baocai Yin, and Xiang Ruan. Learning to detect\nsalient objects with image-level supervision. In CVPR , 2017.\n19\n[83] Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and\nChi-Keung Tang. Fss-1000: A 1000-class dataset for few-shot\nsegmentation. In CVPR , 2020. 19\n[84] Yi Zeng, Pingping Zhang, Jianming Zhang, Zhe Lin, and\nHuchuan Lu. Towards high-resolution salient object detection.\nInICCV , 2019. 19[85] Ho Kei Cheng, Jihoon Chung, Yu-Wing Tai, and Chi-Keung\nTang. Cascadepsp: Toward class-agnostic and very high-\nresolution segmentation via global and local refinement. "}, {"title": "Section 0455", "content": "In\nCVPR , 2020. 6, 19\n[86] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu,\nXiang Bai, Serge Belongie, Alan Yuille, Philip HS Torr, and\nSong Bai. Occluded video instance segmentation: A bench-\nmark. In IJCV , 2022. 6, 15, 16\n[87] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR , 2019. 6\n[88] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Gir-\nshick. Pointrend: Image segmentation as rendering. In CVPR ,\n2020. 6, 20\n[89] Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang,\nPinxue Guo, Zhaoyu Chen, and Wenqiang Zhang. Lvos: A\nbenchmark for long-term video object segmentation. In ICCV ,\n2023. 6, 7, 12, 15, 16, 17\n[90] Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr,\nAndreas Geiger, Laura Leal-Taix\u00e9, and Bastian Leibe. Hota:\nA higher order metric for evaluating multi-object tracking. In\nIJCV , 2021. 6, 14\n[91] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR , 2009. 7, 19\n[92] V olodymyr Kindratenko, Dawei Mu, Yan Zhan, John Mal-\noney, Sayed Hadi Hashemi, Benjamin Rabe, Ke Xu, Roy\nCampbell, Jian Peng, and William Gropp. Hal: Computer\nsystem for scalable deep learning. In PEARC , 2020. 8\n[93] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV , 2014. 16\n[94] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-\nson, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An\nimperative style, high-performance deep learning library. In\nNeurIPS , 2019. 19\n[95] Jean Duchon. Splines minimizing rotation-invariant semi-\nnorms in sobolev spaces. In Constructive Theory of Functions\nof Several Variables , 1977. "}, {"title": "Section 0472", "content": "19\n[96] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-\nYi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple\ncopy-paste is a strong data augmentation method for instance\nsegmentation. In CVPR , 2021. 20\n[97] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Dzmitry Bahdanau,\nand Yoshua Bengio. On the properties of neural machine\ntranslation: Encoder-decoder approaches. In arXiv , 2014. 21\n[98] Konstantin Sofiiuk, Ilya A Petrov, and Anton Konushin. Re-\nviving iterative training with mask guidance for interactive\nsegmentation. In ICIP , 2022. 21\n11Supplementary Material\nPutting the Object Back into Video Object Segmentation\nThe supplementary material is structured as follows:\n1. We first provide visual comparisons of Cutie with state-of-the-art methods in Section A.\n2. We then show some highly challenging cases where both Cutie and state-of-the-art methods fail in Section B.\n3. We analyze the running time of XMem and Cutie in Section C.\n4.To elucidate the workings of the object transformer, we visualize the difference in attention patterns of pixel readout v.s.\nobject readout, feature progression within the object transformer, and the qualitative benefits of masked attention/object\ntransformer in Section D.\n5. We present additional details on BURST evaluation in Section E.\n6.We list options for adjusting the speed-accuracy trade-off without re-training, comparisons with methods that use external\ntraining, additional quantitative results on YouTube-VOS 2018 [ 19]/LVOS [ 89], and the performance variations with respect\nto different random seeds in Section F.\n7. We give more implementation details on the training process, decoder architecture, and pixel memory in Section G.\n8.Lastly, we showcase an interactive video segmentation tool powered by Cutie in Section H. This tool will be open-sourced\nto help researchers and data annotators.\nA. Visual Comparisons\nWe provide visual comparisons of Cutie with DeAOT-R50 [ 68] and XMem [ 9] at youtu.be/LGbJ11GT8Ig. "}, {"title": "Section 0485", "content": "For a fair\ncomparison, we use Cutie-base and train all models with the MOSE dataset. We visualize the comparisons using sequences\nfrom YouTubeVOS-2019 validation, DA VIS 2017 test-dev, and MOSE validation. Only the first-frame (not full-video)\nground-truth annotations are available in these datasets. At the beginning of each sequence, we pause for two seconds to\nshow the first-frame segmentation that initializes all the models. Our model is more robust to distractors and generates more\ncoherent masks.\nB. Failure Cases\nWe visualize some failure cases of Cutie at youtu.be/PIjXUYRzQ8Q, following the format discussed in Section A. As discussed\nin the main paper, Cutie fails in some of the challenging cases where similar objects move in close proximity or occlude each\nother. This problem is not unique to Cutie, as current state-of-the-art methods also fail as shown in the video.\nIn the first sequence \u201celephants\u201d, all models have difficulty tracking the elephants (e.g., light blue mask) behind the big\n(unannotated) foreground elephant. In the second sequence \u201cbirds\u201d, all models fail when the bird with the pink mask moves\nand occludes other birds.\nWe think that this is due to the lack of useful features from the pixel memory and the object memory, as they fail to\ndisambiguate objects that are similar in both appearance and position. A potential future work direction for this is to encode\nthree-dimensional spatial understanding (i.e., the bird that occludes is closer to the camera).\nC. Running Time Analysis\nWe analyze the total runtime of XMem and Cutie in Tab. S1, testing on a single video with a 2080Ti. We synchronize and\nwarm up properly to get accurate timing; small deviations might arise from minor implementation differences and run-time\nvariations. "}, {"title": "Section 0500", "content": "We note that the speedup is mostly achieved by using a lighter decoder.\nXMem Cutie-base Cutie-small\nQuery encoder 0.861 0.851 0.295\nMask encoder 0.143 0.145 0.142\nPixel memory read 0.758 0.514 0.514\nObject memory read - 0.913 0.894\nDecoding 2.749 0.725 0.700\nTable S1. Total running time (s) of each component in XMem and Cutie.\n12Input image Target object mask Pixel attention map Object query attention map\nFigure S1. Comparison of foreground attention patterns between pixel attention with object query attention. In each of the three examples,\nthe two leftmost columns show the input image and the ground-truth (annotated by us for reference). The two rightmost columns show the\nattention patterns for pixel attention and object query attention respectively. Ideally, the attention weights should focus on the foreground\nobject. As shown, the pixel attention has a broader coverage but is easily distracted by similar objects. The object query attention\u2019s attention\nis more sparse (as we use a small number of object queries), and is more focused on the foreground. Our object transformer makes use of\nboth: it first initializes with pixel attention and restructures the features iteratively with object query attention.\nD. Additional Visualizations\nD.1. Attention Patterns of Pixel Attention v.s. Object Query Attention\nHere, we visualize the attention maps of pixel memory reading and of the object transformer, showing the qualitative difference\nbetween the two.\nTo visualize attention in pixel memory reading, we use \u201cself-attention\u201d, i.e., by setting k=q\u2208RHW\u00d7Ck. We compute\nthe pixel affinity Apix\u2208[0,1]HW\u00d7HW, as in Equation (9) of the main paper. We then sum over the foreground region along\nthe rows, visualizing the affinity of every pixel to the foreground. Ideally, all the affinity should be in the foreground \u2013 others\nare distractors that cause erroneous matching. "}, {"title": "Section 0513", "content": "The foreground region is defined by the last auxiliary mask MLin the object\ntransformer.\nTo visualize the attention in the object transformer, we inspect the attention weights AL\u2208RN\u00d7HWof the first (pixel-to-\nquery) cross-attention in the last object transformer block. Similar to how we visualize the pixel attention, we focus on the\nforeground queries (i.e., first N/2object queries) and sum over the corresponding rows in the affinity matrix.\nFigure S1 visualizes the differences between these two types of attention. The pixel attention is more spread out and is\neasily distracted by similar objects. The object query attention focuses on the foreground without being distracted. Our object\ntransformer makes use of both types of attention by using pixel attention for initialization and object query attention for\nrestructuring the feature in every transformer block.\nD.2. Feature Progression in the Object Transformer\nFigure S2 visualizes additional feature progressions within the object transformer (in addition to Figure 4 in the main paper).\nThe object transformer helps to suppress noises from low-level matching and produces more coherent object-level features.\nD.3. Benefits of Masked Attention/Object Transformer\nFigure S3 qualitatively compares results with/without using masked attention \u2013 while both work well for simpler cases, masked\nattention helps in challenging cases with similar objects. Figure S4 visualizes the benefits of the object transformer. Using the\nobject transformer leads to more complete and accurate outputs.\n13Image M1 M2 M3 Ground-truth\nFigure S2. Visualization of auxiliary masks Mlin the l-th object transformer block. At every layer, matching errors are suppressed (pink\narrows) and the target object becomes more coherent (yellow arrows). The ground-truth is annotated by us for reference.\nw/o masked attn w/ masked attnSimple\n Cluttered\nFigure S3. Comparisons of Cutie with/without masked attention. "}, {"title": "Section 0528", "content": "While both work well in simple cases, masked attention helps to\ndifferentiate similarly-looking objects.\nE. Details on BURST Evaluation\nIn BURST [ 3], we update the memory every 10th frame following [ 9]. Since BURST contains high-resolution images (e.g.,\n1920\u00d71200), we downsize the images such that the shorter edge has no more than 600 pixels instead of the default 480 pixels\nfor all methods. Following [ 3], we assess Higher Order Tracking Accuracy (HOTA) [ 90] on common and uncommon object\nclasses separately.\nFor better performance on long videos, we experiment with the long-term memory [ 9] in addition to our default FIFO\nmemory strategy. The long-term memory is a plug-in addition to our pixel memory \u2013 it routinely compresses the attentional\n\u201cworking memory\u201d into a long-term memory storage instead of discarding them as in our first-in-first-out approach. The\nlong-term memory can be adopted without any re-training. We follow the default long-term memory parameters in XMem [ 9]\nand present the improvement in the main paper.\n14Figure S4. Top-to-bottom: Without object queries, Cutie\u2019s default model, and ground-truth. The leftmost frame is a reference frame.\nF. Additional Quantitative Results\nF.1. Speed-Accuracy Trade-off\nWe note that the performance of Cutie can be further improved by changing hyperparameters like memory interval and the\nsize of the memory bank during inference, at the cost of a slower running time. Here, we present \u201cCutie+\u201d, which adjusts the\nfollowing hyperparameters without re-training:\n1. Maximum memory frames Tmax= 5\u2192Tmax= 10\n2. Memory interval r= 5\u2192r= 3\n3. Maximum shorter side resolution during inference 480\u2192720pixels\nThese settings apply to DA VIS [ 13] and MOSE [ 12]. "}, {"title": "Section 0539", "content": "For YouTubeVOS, we keep the memory interval r= 5 and set the\nmaximum shorter side resolution during inference to 600for two reasons: 1) YouTubeVOS is annotated every 5 frames, and\naligning the memory interval with annotation avoids adding unannotated objects into the memory as background, and 2)\nYouTubeVOS has lower video quality and using higher resolution makes artifacts more apparent. The results of Cutie+ are\ntabulated in the bottom portion of Table S2.\nF.2. Comparisons with Methods that Use External Training\nHere, we present comparisons with methods that use external training: SimVOS [ 81], JointFormer [ 70], and ISVOS [ 76] in\nTable S2. Note, we could not obtain the code for these methods at the time of writing. ISVOS [ 76] does not report running\ntime \u2013 we estimate to the best of our ability with the following information: 1) For the VOS branch, it uses XMem [ 9] as\nthe baseline with a first-in-first-out 16-frame memory bank, 2) for the instance branch, it uses Mask2Former [ 15] with an\nunspecified backbone. Beneficially for ISVOS, we assume the lightest backbone (ResNet-50), and 3) the VOS branch and the\ninstance branch share a feature extraction backbone. Our computation is as follows:\n1. Time per frame for XMem with a 16-frame first-in-first-out memory bank (from our testing): 75.2 ms\n2. Time per frame for Mask2Former with ResNet-50 backbone (from Mask2Former paper): 103.1 ms\n3. Time per frame of the doubled-counted feature extraction backbone (from our testing): 6.5 ms\nThus, we estimate that ISVOS would take (75.2+103.1-6.5) = 171.8 ms per frame, which translates to 5.8 frames per second.\nIn an endeavor to reach a better performance with Cutie by adding more training data, we devise a \u201cMEGA\u201d training\nscheme that includes training on BURST [ 3] and OVIS [ 86] in addition to DA VIS [ 13], YouTubeVOS [ 19], and MOSE [ 12].\nWe train for an additional 50K iterations in the MEGA setting. The results are tabulated in the bottom portion of Table S2.\nF.3. "}, {"title": "Section 0540", "content": "Results on YouTubeVOS-2018 and LVOS\nHere, we provide additional results on the YouTubeVOS-2018 validation set and LVOS [ 89] validation/test sets in Table S3.\nFPS is measured on YoutubeVOS-2018/2019 following the main paper. "}, {"title": "Section 0541", "content": "YouTubeVOS-2018 is the old version of YouTubeVOS-\n15MOSE DA VIS-17 val DA VIS-17 test YouTubeVOS-2019 val\nMethod J&FJFJ&FJFJ&FJFGJsFsJuFuFPS\nSimVOS-B [69] - - - 81.3 78.8 83.8 - - - - - - - - 3.3\nSimVOS-B [69] w/ MAE [71] - - - 88.0 85.0 91.0 80.4 76.1 84.6 84.2 83.1 -79.1 - 3.3\nJointFormer [70] - - - - - - 65.6 61.7 69.4 73.3 75.2 78.5 65.8 73.6 3.0\nJointFormer [70] w/ MAE [71] - - - 89.7 86.7 92.7 87.6 84.2 91.1 87.0 86.1 90.6 82.0 89.5 3.0\nJointFormer [70] w/ MAE [71] + BL30K [2] - - - 90.1 87.0 93.2 88.1 84.7 91.6 87.4 86.5 90.9 82.0 90.3 3.0\nISVOS [76] - - - 80.0 76.9 83.1 - - - - - - - -5.8\u2217\nISVOS [76] w/ COCO [93] - - - 87.1 83.7 90.5 82.8 79.3 86.2 86.1 85.2 89.7 80.7 88.9 5.8\u2217\nISVOS [76] w/ COCO [93] + BL30K [2] - - - 88.2 84.5 91.9 84.0 80.1 87.8 86.3 85.2 89.7 81.0 89.1 5.8\u2217\nCutie-small 62.2 58.2 66.2 87.2 84.3 90.1 84.1 80.5 87.6 86.2 85.3 89.6 80.9 89.0 45.5\nCutie-base 64.0 60.0 67.9 88.8 85.4 92.3 84.2 80.6 87.7 86.1 85.5 90.0 80.6 88.3 36.4\nCutie-small w/ MOSE [12] 67.4 63.1 71.7 86.5 83.5 89.5 83.8 80.2 87.5 86.3 85.2 89.7 81.1 89.2 45.5\nCutie-base w/ MOSE [12] 68.3 64.2 72.3 88.8 85.6 91.9 85.3 81.4 89.3 86.5 85.4 90.0 81.3 89.3 36.4\nCutie-small w/ MEGA 68.6 64.3 72.9 87.0 84.0 89.9 85.3 81.4 89.2 86.8 85.2 89.6 82.1 90.4 45.5\nCutie-base w/ MEGA 69.9 65.8 74.1 87.9 84.6 91.1 86.1 82.4 89.9 87.0 86.0 90.5 82.0 89.6 36.4\nCutie-small+ 64.3 60.4 68.2 88.7 86.0 91.3 85.7 82.5 88.9 86.7 85.7 89.8 81.7 89.6 20.6\nCutie-base+ 66.2 62.3 70.1 90.5 87.5 93.4 85.9 82.6 89.2 86.9 86.2 90.7 81.6 89.2 17.9\nCutie-small+ w/ MOSE [12] 69.0 64.9 73.1 89.3 86.4 92.1 86.7 83.4 90.1 86.5 85.4 89.7 81.6 89.2 20.6\nCutie-base+ w/ MOSE [12] 70.5 66.5 74.6 90.0 87.1 93.0 86.3 82.9 89.7 86.8 85.7 90.0 81.8 89.6 17.9\nCutie-small+ w/ MEGA 70.3 66.0 74.5 89.3 86.2 92.5 87.1 83.8 90.4 86.8 85.4 89.5 82.3 90.0 20.6\nCutie-base+ w/ MEGA 71.7 67.6 75.8 88.1 85.5 90.8 88.1 84.7 91.4 87.5 86.3 90.6 82.7 90.5 17.9\nTable S2. "}, {"title": "Section 0550", "content": "Quantitative comparison on common video object segmentation benchmarks, including methods that use external training\ndata. Recent vision-transformer-based methods [ 69,70,76] depend largely on pretraining, either with MAE [ 71] or pretraining a separate\nMask2Former [ 15] network on COCO instance segmentation [ 93]. Note they do not release code at the time of writing, and thus they cannot\nbe reproduced on datasets that they do not report results on. Cutie performs competitively to those recent (slow) transformer-based methods,\nespecially with added training data. MEGA is the aggregated dataset consisting of DA VIS [ 13], YouTubeVOS [ 19], MOSE [ 12], OVIS [ 86],\nand BURST [3].\u2217estimated FPS.\nYouTubeVOS-2018 val LVOS val LVOS test\nMethod GJsFsJuFuJ&FJFJ&FJF FPS\nDEV A [5] 85.9 85.5 90.1 79.7 88.2 58.3 52.8 63.8 54.0 49.0 59.0 25.3\nDEV A [5] w/ MOSE [12] 85.8 85.4 90.1 79.7 88.2 55.9 51.1 60.7 56.5 52.2 60.8 25.3\nDDMemory [89] 84.1 83.5 88.4 78.1 86.5 60.7 55.0 66.3 55.0 49.9 60.2 18.7\nCutie-small 86.3 85.5 90.1 80.6 89.0 58.8 54.6 62.9 57.2 53.7 60.7 45.5\nCutie-base 86.1 85.8 90.5 80.0 88.0 60.1 55.9 64.2 56.2 51.8 60.5 36.4\nCutie-small w/ MOSE [12] 86.8 85.7 90.4 81.6 89.7 60.7 55.6 65.8 56.9 53.5 60.2 45.5\nCutie-base w/ MOSE [12] 86.6 85.7 90.6 80.8 89.1 63.5 59.1 67.9 63.6 59.1 68.0 36.4\nCutie-small w/ MEGA 86.9 85.5 90.1 81.7 90.2 62.9 58.3 67.4 66.4 61.9 70.9 45.5\nCutie-base w/ MEGA 87.0 86.4 91.1 81.4 89.2 66.0 61.3 70.6 66.7 62.4 71.0 36.4\nTable S3. Quantitative comparison on YouTubeVOS-2018 [ 19] and LVOS [ 89]. DDMemory [ 89] is the baseline method presented in\nLVOS [ 89] with no available official code at the time of writing. Note, we think LVOS is significantly different than other datasets because it\ncontains a lot more tiny objects. See Section F.3 for details. "}, {"title": "Section 0566", "content": "MEGA is the aggregated dataset consisting of DA VIS [ 13], YouTubeVOS [ 19],\nMOSE [12], OVIS [86], and BURST [3].\n2019 \u2013 we present our main results using YouTubeVOS-2019 and provide results on YouTubeVOS-2018 for reference. Note\nthat these results are ready at the time of paper submission and are referred to in the main paper. The complete tables are listed\nhere due to space constraints in the main paper.\nLVOS [ 89] is a recently proposed long-term video object segmentation benchmark, with 50 videos in its validation set\nand test set respectively. Note, we have also presented results in another long-term video object segmentation benchmark,\nBURST [ 3] in the main paper, which contains 988 videos in the validation set and 1419 videos in the test set. We test Cutie on\nLVOS after completing the design of Cutie, adopt long-term memory [ 9], and perform no tuning. We note that our method\n16BURST val BURST test\nMethod All Com. Unc. All Com. Unc. Memory usage\nDeAOT [68] FIFO w/ MOSE [12] 51.3 56.3 50.0 53.2 53.5 53.2 10.8G\nDeAOT [68] INF w/ MOSE [12] 56.4 59.7 55.5 57.9 56.7 58.1 34.9G\nXMem [9] FIFO w/ MOSE [12] 52.9 56.0 52.1 55.9 57.6 55.6 3.03G\nXMem [9] LT w/ MOSE [12] 55.1 57.9 54.4 58.2 59.5 58.0 3.34G\nCutie-small FIFO w/ MOSE [12] 56.8 61.1 55.8 61.1 62.4 60.8 1.35G\nCutie-small LT w/ MOSE [12] 58.3 61.5 57.5 61.6 63.1 61.3 2.28G\nCutie-base LT w/ MOSE [12] 58.4 61.8 57.5 62.6 63.8 62.3 2.36G\nCutie-small LT w/ MEGA 61.6 65.3 60.6 64.4 63.7 64.6 2.28G\nCutie-base LT w/ MEGA 61.2 65.0 60.3 66.0 66.5 65.9 2.36G\nTable S4. Extended comparisons of performance on long videos on the BURST dataset [ 3], including our results when trained in the\nMEGA setting. Com. and Unc. stand for common and uncommon objects respectively. Mem.: maximum GPU memory usage. FIFO:\nfirst-in-first-out memory bank; INF: unbounded memory; LT: long-term memory [ 9]. "}, {"title": "Section 0576", "content": "DeAOT [ 68] is not compatible with long-term memory.\n0 20 40 60 80 100\nPercentage of annotated pixels (%)0.00.20.40.60.81.0Cumulative normalized frequencyDAVIS\nYouTubeVOS\nMOSE\nBURST\nLVOS\nFigure S5. Cumulative frequency graph of annotated pixel areas (as percentages of the total image area) for different datasets. Tiny objects\nare significantly more prevalent on LVOS [89] than on other datasets.\n(Cutie-base) performs better than DDMemory, the baseline presented in LVOS [ 89], on the test set and has a comparable\nperformance on the validation set, while running about twice as fast. Upon manual inspection of the results, we observe\nthat one of the unique challenges in LVOS is the prevalence of tiny objects, which our algorithm has not been specifically\ndesigned to handle. We quantify this observation by analyzing the first frame annotations of all the videos in the validation\nsets of DA VIS [ 13], YouTubeVOS [ 19], MOSE [ 12], BURST [ 3], and LVOS [ 89], as shown in Figure S5. Tiny objects\nare significantly more prevalent on LVOS [ 89] than on other datasets. We think this makes LVOS uniquely challenging for\nmethods that are not specifically designed to detect small objects.\n171 2 3 4 5 6 7 80204060\nNumber of objectsFrames per second (FPS)\nFigure S6. Cutie-small\u2019s processing speed with respect to the number of objects in the video. Common benchmarks (DA VIS [ 13],\nYouTubeVOS [ 19], and MOSE [ 12]) average 2-3 objects per video with longer-term benchmarks like BURST [ 3] averaging 5.57 objects per\nvideo \u2013 our model remains real-time (25+ FPS) in these scenarios. For evaluation, we use standard 854\u00d7480test videos with 100 frames\neach.\nF.4. "}, {"title": "Section 0588", "content": "Performance Variations\nTo assess performance variations with respect to different random seeds, we train Cutie-small with five different random seeds\n(including both pretraining and main training with the MOSE dataset) and report mean \u00b1standard deviation on the MOSE [ 12]\nvalidation set and the YouTubeVOS 2019 [ 19] validation set in Table S5. Note, the improvement brought by our model (i.e.,\n8.7J&Fon MOSE and 0.9 Gon YouTubeVOS over XMem [9]) corresponds to +24.2s.d. and +8.2s.d. respectively.\nMOSE val YouTubeVOS-2019 val\nJ&F J F G J s Fs Ju Fu\n67.3\u00b10.36 63.1 \u00b10.36 71.6 \u00b10.35 86.2 \u00b10.11 85.1 \u00b10.20 89.6 \u00b10.27 81.1 \u00b10.19 89.3 \u00b10.13\nTable S5. Performance variations (median \u00b1standard deviation) across five different random seeds.\nG. Implementation Details\nHere, we include more implementation details for completeness. Our training and testing code will be released for repro-\nducibility.\nG.1. Extension to Multiple Objects\nWe extend Cutie to the multi-object setting following [ 5,8,9,51]. Objects are processed independently (in parallel as a batch)\nexcept for 1) the interaction at the first convolutional layer of the mask encoder, which extracts features corresponding to a\ntarget object with a 5-channel input concatenated from the image (3-channel), the mask of the target object (1-channel), and\nthe sum of masks of all non-target objects (1-channel); 2) the interaction at the soft-aggregation layers [ 8] used to generate\nsegmentation logits \u2013 where the object probability distributions at every pixel are normalized to sum up to one. Note these are\nstandard operations from prior works [ 5,8,9,51]. Parts of the computation (i.e., feature extraction from the query image and\naffinity computation) are shared between objects while the rest are not. We experimented with object interaction within the\nobject transformer in the early stage of this project but did not obtain positive results.\nFigure S6 plots the FPS against the number of objects. "}, {"title": "Section 0599", "content": "Our method slows down with more objects but remains real-time\nwhen handling a common number of objects in a scene (29.9 FPS with 5 objects). For instance, the BURST [ 3] dataset\naverages 5.57 object tracks per video and DA VIS-2017 [13] averages just 2.03.\nAdditionally, we plot the memory usage with respect to the number of processed frames during inference in Figure S7.\n18100 500 1,000 1,500 2,000 2,500 3,000102103104\nNumber of processed framesGPU memory usage (MB)Ours (first-in-first-out)\nOurs (long-term memory)\nXMem (long-term memory)\nDeAOT (unbounded memory)\nFigure S7. Running GPU memory usage (log-scale) comparisons with respect to the number of processed frames during inference. By\ndefault, we use a first-in-first-out (FIFO) memory bank which leads to constant memory usage over time. Optionally, we include the\nlong-term memory from XMem [ 9] in our method for better performance on long videos. Our method (with long-term memory) uses less\nmemory than XMem because of a smaller channel size (256 in our model; 512 in XMem). DeAOT [ 68] has an unbounded memory size and\nis impractical for processing long videos \u2013 our hardware (32GB V100, server-grade GPU) cannot process beyond 1,400 frames.\nG.2. Streaming Average Algorithm for the Object Memory\nTo recap, we store a compact set of Nvectors which make up a high-level summary of the target object in the object memory\nS\u2208RN\u00d7C. At a high level, we compute Sby mask-pooling over all encoded object features with Ndifferent masks.\nConcretely, given object features U\u2208RTHW\u00d7CandNpooling masks {Wq\u2208[0,1]THW,0< q\u2264N}, where Tis the\nnumber of memory frames, the q-th object memory Sq\u2208RCis computed by\nSq=PTHW\ni=1U(i)Wq(i)\nPTHW\ni=1Wq(i). (S1)\nDuring inference, we use a classic streaming average algorithm such that this operation takes constant time and memory\nwith respect to the memory length. Concretely, for the q-th object memory at time step t, we keep track of a cumulative\nmemory \u03c3t\nSq\u2208RCand a cumulative weight \u03c3t\nWq\u2208R. "}, {"title": "Section 0611", "content": "We update the accumulators and find Sqvia\n\u03c3t\nSq=\u03c3t\u22121\nSq+THWX\ni=1U(i)Wq(i), \u03c3t\nWq=\u03c3t\u22121\nWq+THWX\ni=1Wq(i), and Sq=\u03c3t\nSq\n\u03c3t\nWq, (S2)\nwhere UandWqcan be discarded after every time step.\nG.3. Training Details\nAs mentioned in the main paper, we train our network in two stages: static image pretraining and video-level main training\nfollowing prior works [ 8\u201310]. Backbone weights are initialized from ImageNet [ 91] pretraining, following prior work [ 8\u201310].\nWe implement our network with PyTorch [94] and use automatic mixed precision (AMP) for training.\nG.3.1 Pretraining\nOur pretraining pipeline follows the open-sourced code of [ 2,9,51], and is described here for completeness. For pretraining,\nwe use a set of image segmentation datasets: ECSSD [ 81], DUTS [ 82], FSS-1000 [ 83], HRSOD [ 84], and BIG [ 85]. We\nmix these datasets and sample HRSOD [ 84] and BIG [ 85] five times more often than the others as they are more accurately\nannotated. From a sampled image-segmentation pair, we generate synthetic sequences of length three by deforming the pair\nwith random affine transformation, thin plate spline transformation [ 95], and cropping (with crop size 384\u00d7384). With the\ngenerated sequence, we use the first frame (with ground-truth segmentation) as the memory frame to segment the second\nframe. Then, we encode the second frame with our predicted segmentation and concatenate it with the first-frame memory to\nsegment the third frame. Loss is computed on the second and third frames and back-propagated through time.\n19G.3.2 Main Training\nFor main training, we use two different settings. The \u201cwithout MOSE\u201d setting mixes the training sets of DA VIS-2017 [ 13] and\nYouTubeVOS-2019 [ 19]. The \u201cwith MOSE\u201d setting mixes the training sets of DA VIS-2017 [ 13], YouTubeVOS-2019 [ 19], and\nMOSE [ 12]. In both settings, we sample DA VIS [ 13] two times more often as its annotation is more accurate. "}, {"title": "Section 0620", "content": "To sample a\ntraining sequence, we first randomly select a \u201cseed\u201d frame from all the frames and randomly select seven other frames from the\nsame video. We re-sample if any two consecutive frames have a temporal frame distance above D. We employ a curriculum\nlearning schedule following [ 9] forD, which is set to [5,10,15,5]correspondingly after [0%,10%,30%,80%] of training\niterations.\nFor data augmentation, we apply random horizontal mirroring, random affine transformation, cut-and-paste [ 96] from\nanother video, and random resized crop (scale [0.36,1.00], crop size 480\u00d7480). We follow stable data augmentation [ 5] to\napply the same crop and rotation to all the frames in the same sequence. We additionally apply random color jittering and\nrandom grayscaling to the sampled images following [9, 51].\nTo train on a sampled sequence, we follow the process of pretraining, except that we only use a maximum of three memory\nframes to segment a query frame following [ 9]. When the number of past frames is smaller or equal to 3, we use all of them,\notherwise, we randomly sample three frames to be the memory frames. We compute the loss at all frames except the first one\nand back-propagate through time.\nG.3.3 Point Supervision\nAs mentioned in the main paper, we adopt point supervision [ 15] for training. As reported by [ 15], using point supervision for\ncomputing the loss has insignificant effects on the final performance while using only one-third of the memory during training.\nIn Cutie, we note that using point supervision reduces the memory cost during loss computation but has an insignificant impact\non the overall memory usage. We use importance sampling with default parameters following [ 15], i.e., with an oversampling\nratio of 3, and sample 75% of all points from uncertain points and the rest from a uniform distribution. "}, {"title": "Section 0637", "content": "We use the uncertainty\nfunction for semantic segmentation (by treating each object as an object class) from [ 88], which is the logit difference between\nthe top-2 predictions. Note that using point supervision also focuses the loss in uncertain regions but this is not unique to our\nframework. Prior works XMem [ 9] and DeAOT [ 68] use bootstrapped cross-entropy to similarly focus on difficult-to-segment\npixels. Overall, we do not notice significant segmentation accuracy differences in using point supervision vs. the loss in\nXMem [9].\nG.4. Decoder Architecture\nOur decoder design follows XMem [ 9] with a reduced number of channels. XMem [ 9] uses 256 channels while we use 128\nchannels. This reduction in the number of channels improves the running time. We do not observe a performance drop from\nthis reduction which we think is attributed to better input features (which are already refined by the object transformer).\nThe inputs to the decoder are the object readout feature RLat stride 16 and skip-connections from the query encoder at\nstrides 8 and 4. The skip-connection features are first projected to Cdimensions with a 1\u00d71convolution. We process the\nobject readout features with two upsampling blocks and incorporate the skip-connections for high-frequency information in\neach block. In each block, we first bilinearly upsample the input feature by two times, then add the upsampled features with\nthe skip-connection features. This sum is processed by a residual block [ 80] with two 3\u00d73convolutions as the output of the\nupsample block. In the final layer, we use a 3\u00d73convolution to predict single-channel logits for each object. The logits are\nbilinearly upsampled to the original input resolution. In the multi-object scenario, we use soft-aggregation [ 8] to merge the\nobject logits.\nG.5. Details on Pixel Memory\nAs discussed in the main paper, we derive our pixel memory design from XMem [ 9] without claiming contributions. "}, {"title": "Section 0647", "content": "Namely,\nthe attentional component is derived from the working memory, and the recurrent component is derived from the sensory\nmemory of XMem [ 9]. Long-term memory [ 9], which compresses the working memory during inference, can be adopted\nwithout re-training for evaluation on long videos.\nG.5.1 Attentional Component\nFor the attentional component, we store memory keys k\u2208RTHW\u00d7Ckand memory value v\u2208RTHW\u00d7Cand later retrieve\nfeatures using a query key q\u2208RHW\u00d7Ck. Here, Tis the number of memory frames and H, W are image dimensions at stride\n2016. As we use the anisotropic L2 similarity function [ 9], we additionally store a memory shrinkage s\u2208[1,\u221e]THWterm and\nuse a query selection term e\u2208[0,1]HW\u00d7Ckduring retrieval.\nThe anisotropic L2 similarity function d(\u00b7,\u00b7)is computed as\nd(qi,kj) =\u2212sjCkX\nceic(kic\u2212qjc). (S3)\nWe compute memory keys k, memory shrinkage terms s, query keys q, and query selection terms eby projecting features\nencoded from corresponding RGB images using the query encoder. Since these terms are only dependent on the image, they,\nand thus the affinity matrix Apixcan be shared between multiple objects with no additional costs. The memory value vis\ncomputed by fusing features from the mask encoder (that takes both image and mask as input) and the query encoder. This\nfusion is done by first projecting the input features to Cdimensions with 1\u00d71convolutions, adding them together, and\nprocessing the sum with two residual blocks, each with two 3\u00d73convolutions.\nG.5.2 Recurrent Component\nThe recurrent component stores a hidden state hHW\u00d7Cwhich is updated by a Gated Recurrent Unit (GRU) [ 97] every\nframe. This GRU takes multi-scale inputs (from stride 16, 8, and 4) from the decoder to update the hidden state h. We first\narea-downsample the input features to stride 16, then project them to Cdimensions before adding them together. "}]]