["ZBS: Zero-shot Background Subtraction via Instance-level Background", [{"title": "Section 0013", "content": "ZBS: Zero-shot Background Subtraction via Instance-level Background\nModeling and Foreground Selection\nYongqi An1,2Xu Zhao1,*Tao Yu1,2Haiyun Guo1,2\nChaoyang Zhao1Ming Tang1,2Jinqiao Wang1,2\nNational Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China1\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China2\n{yongqi.an,xu.zhao,haiyun.guo,tangm,jqwang }@nlpr.ia.ac.cn\nyutao2022@ia.ac.cn\nAbstract\nBackground subtraction (BGS) aims to extract all mov-\ning objects in the video frames to obtain binary foreground\nsegmentation masks. Deep learning has been widely used in\nthis field. Compared with supervised-based BGS methods,\nunsupervised methods have better generalization. However,\nprevious unsupervised deep learning BGS algorithms per-\nform poorly in sophisticated scenarios such as shadows or\nnight lights, and they cannot detect objects outside the pre-\ndefined categories. In this work, we propose an unsuper-\nvised BGS algorithm based on zero-shot object detection\ncalled Zero-shot Background Subtraction (ZBS). The pro-\nposed method fully utilizes the advantages of zero-shot ob-\nject detection to build the open-vocabulary instance-level\nbackground model. Based on it, the foreground can be ef-\nfectively extracted by comparing the detection results of new\nframes with the background model. ZBS performs well for\nsophisticated scenarios, and it has rich and extensible cat-\negories. Furthermore, our method can easily generalize to\nother tasks, such as abandoned object detection in unseen\nenvironments. We experimentally show that ZBS surpasses\nstate-of-the-art unsupervised BGS methods by 4.70 %F-\nMeasure on the CDnet 2014 dataset. The code is released\nathttps://github.com/CASIA-IVA-Lab/ZBS .\n1. Introduction\nBackground subtraction (BGS) is a fundamental task\nin computer vision applications [7], such as autonomous\nnavigation, visual surveillance, human activity recognition,\netc[15]. "}, {"title": "Section 0025", "content": "BGS aims to extract all moving objects as fore-\nground in each video frame and outputs binary segmenta-\n*Corresponding Author\nTraditional method\n(SuBSENSE)Ground Truth\nDeep learning-based\nunsupervised method\n(RT-SBS-v2)ZBS (Ours)\nDeep learning-based\nsupervised method\n(BSUV -Net 2.0)Pixel-level BGSInstance-level\nBGS\nShadowPTZCamera JitterInput frame\nFigure 1. The performance of different BGS methods. Previous\nBGS methods based on pixel-level background models may mis-\njudge noisy background as foreground objects, such as camera-\nJitter, PTZ, and shadow. Our method based on an instance-level\nbackground model can obtain precise foreground edges, effec-\ntively reducing the confusion of background pixels as foreground\nobjects.\ntions.\nThe most straightforward BGS algorithm is to directly\ncompare the current frame with the \u201dstationary\u201d background\nimage [7]. However, this strategy cannot handle com-\nplex scenarios, such as dynamic background, illumination\nchanges, and shadows. Therefore, more sophisticated BGS\ntechniques [7, 20, 24, 48] have been proposed in the past\ndecades. The traditional methods improve performance in\ntwo aspects. The first is to design more robust feature repre-\nsentations, including color features [44], edge features [20],\nmotion features [48], and texture features [12]. The sec-\nond is to design more suitable background models, such\nas Gaussian mixture models [36], kernel density estimation\nmodels [14], CodeBook [21], ViBe [4], SuBSENSE [34],\nand PAWCS [35]. The traditional methods have relatively\nadequate generalization capacity since they are not opti-\nmized on specific scenarios or categories of objects. How-\n1arXiv:2303.14679v1  [cs.CV]  26 Mar 2023ever, these methods only utilize hand-craft features to deter-\nmine whether each pixel belongs to the foreground. We call\nthese methods pixel-level BGS since they use pixel-based\nor local pixels-based background models. "}, {"title": "Section 0038", "content": "They are sensi-\ntive to natural variations such as lighting and weather.\nOver the years, deep learning-based BGS algorithms\nhave been proposed, including supervised BGS and unsu-\npervised BGS. Supervised BGS algorithms have achieved\nsatisfactory performance on CDnet 2014 benchmark [11,\n24, 31, 41, 46]. However, these methods usually have to be\ntrained on the first several frames of the test videos, which\nlimits the application to unseen scenarios. Unsupervised\nalgorithms overcome this shortcoming. Most of them com-\nbine semantic segmentation models into traditional BGS al-\ngorithms. These algorithms pre-select 12 categories as fore-\nground from 150 categories of semantic segmentation mod-\nels [9]. Existing state-of-the-art unsupervised methods still\ndetect night light and heavy shadows as foreground objects.\nAs shown in Figure 1, it is difficult for pixel-level back-\nground model to accurately distinguish the edges of fore-\nground objects.\nTo tackle the above problems, we propose a novel back-\nground subtraction framework based on zero-shot object\ndetection (ZBS). The zero-shot object detection, or also\nnamed open-vocabulary object detection, aims to detect un-\nseen objects outside of the pre-defined categories [49]. Fig-\nure 2 shows the framework of our method. The method\nincludes all-instance detection, instance-level background\nmodeling, and foreground instance selection. In the all-\ninstance detection stage, any zero-shot detector can be used.\nWe use a zero-shot object detection model named Detic [49]\nas the all-instance detector to transform the raw image pix-\nels into structured instance representations, including cat-\negories, boxes, and masks. In the background model-\ning stage, our method builds an instance-level background\nmodel based on the motion information of instances. If an\nobject is stationary, our algorithm adds it to the background\nmodel. "}, {"title": "Section 0054", "content": "In the foreground instance selection stage, the pro-\nposed algorithm selects the output of the all-instance detec-\ntor when the new frame comes. If the instance complies\nwith Rule 2 in Figure 2 (c), it is the foreground in the fi-\nnal binary mask. Benefiting from the full use of instance\ninformation, our instance-level BGS method performs bet-\nter in complex scenarios, such as shadows, camera jitter,\nnight scenes, etc. ZBS rarely detects noisy background as\nforeground objects by mistake. Due to the characteristics\nof the detector, the proposed method can detect most of the\ncategories in the real world and can detect the unseen fore-\nground categories outside the pre-defined categories. ZBS\nachieves remarkably 4.70 %F-Measure improvements over\nstate-of-the-art unsupervised methods.\nOur main contributions are listed as follows:\n\u2022 We propose a novel background subtraction frame-work that has the instance-level background model;\n\u2022 The proposed framework uses a zero-shot object de-\ntection model to obtain a more general and generalized\ndeep learning-based unsupervised BGS algorithm;\n\u2022 Our method achieves the state-of-the-art in all unsu-\npervised BGS methods on the CDnet 2014 dataset.\n2. Related work\n2.1. Deep learning-based Supervised Methods\nDeep learning methods have been widely used for BGS\ndue to their ability to learn high-level representations from\ntraining data [8]. Braham et al. [10] presented the first work\nusing deep learning for background subtraction. FgSeg-\nNet [24] is a representative work that focuses on learn-\ning multi-scale features for foreground segmentation. Cas-\ncadeCNN [41] employs a cascade structure to synthesize\nthe basic CNN model and the multi-scale CNN model.\nZhao et al. [46] propose an end-to-end two-stage deep CNN\nto reconstruct the background and separate the foreground\nfrom the background jointly. Chen et al. [11] and Sakkos\net al. [31] use ConvLSTM and 3DCNN, respectively, to\nprocess spatio-temporal information. "}, {"title": "Section 0062", "content": "In addition, Siamese\nneural networks [18, 32], generative adversarial networks\n(GAN) [1,2,47], and autoencoders (AE) [33] have also been\nemployed for BGS.\nRecently, [37, 38, 42, 45] demonstrated better generality\nfor unseen videos with training on limited data. However,\nthese models are trained only on datasets containing a few\ncategories and scenes, limiting their ability to cope with\nmore complex real-world detection and segmentation tasks.\n2.2. Semantic background subtraction\nSemanticBGS [9] is the first motion detection framework\nto utilize object-level semantics for improving background\nsubtraction. By combining semantic segmentation and\nbackground subtraction algorithms, it significantly reduces\nfalse positive detections and effectively identifies camou-\nflaged foreground objects. RTSS [43] performs foreground\ndetection and semantic segmentation in a parallel manner,\nusing the semantic probability of pixels to guide the con-\nstruction and update of the background model. This method\nachieves real-time semantic background subtraction. RT-\nSBS [13] adopts a similar approach and improves the per-\nformance, achieving a real-time semantic background sub-\ntraction algorithm at 25 frames per second.\nDespite their advancements, semantic background sub-\ntraction methods are still fundamentally pixel-level back-\nground models. All of these semantic BGS methods ne-\ncessitate a predefined list of foreground classes, which re-\nquire expert knowledge and pose challenges for implemen-\ntation in various scenarios. "}, {"title": "Section 0074", "content": "Furthermore, the limited num-\nber of categories in semantic segmentation networks (up to\n2\uff1c\n \uff1c\nRule 2: foreground selectionis from the new frame.\nis from the background model.\nHere is a train.\nHere is a doormat.(b) Instance-level Backgr ound ModelingInitial frames\nFinal result\nNew frame\n(c) For eground Instance Selection  box headcls head\nbackbonemask  \nheadRiRi\u00b7B1 Ri\u00b7B2 Ri\u00b7B3 Ri\u00b7B4\u00b7\u00b7\u00b7Ri\u00b7BkB1 B2 B3B4 \u00b7\u00b7\u00b7 BkCLIP Open V ocabulary \ncls_name\nAll-instance detection result(a) All-instance Detection  \nperson\npersonsubway train\nMove information\nAll-instance\nDetection  Instance-level Background Model\u2264\nRule 1: background modeling\nFigure 2. The framework of ZBS. (a) All-instance detection. We use a zero-shot object detection model named Detic [49] to transform\nthe pixel-level image into a structured-instance representation, including categories, boxes, and masks. Specifically, the categories are\nobtained by CLIP. (b) Instance-level background modeling. The proposed method analyzes the motion information of instances. If the\ninstance complies with Rule 1, the stationary instance will be added to the background model. (c) The new frame output by Detic will be\ncompared with the instance-level background model. If the instance complies with Rule 2, it will be the foreground in the final result.\n150 categories) hinders their ability to detect moving fore-\ngrounds in an open-vocabulary setting, an aspect that is be-\ncoming increasingly important in today\u2019s environment.\nThe proposed ZBS builds an instance-level background\nmodel capable of detecting most real-world categories with-\nout the need for predefined foreground classes, thus offering\ngreater practicality.\n2.3. Zero-shot object detection\nIn the era of deep learning, supervised learning has\nachieved outstanding performance on many tasks, but too\nmuch training data has to be used. Moreover, these meth-\nods can not classify or detect the objects of categories out-\nside the training datasets\u2019 annotations. "}, {"title": "Section 0091", "content": "To solve this prob-\nlem, zero-shot learning [3] was proposed, hoping to clas-\nsify images of categories that were never seen in the train-ing process. The zero-shot object detection developed from\nthis aims to detect objects outside the training vocabulary.\nEarlier work studied exploiting attributes to encode cate-\ngories as vectors of binary attributes and learning label em-\nbeddings [3]. The primary solution in Deep Learning is to\nreplace the last classification layer with a language embed-\nding of class names (e.g., GloVe [27]). Rahman et al. [29]\nand Li et al. [23] improve by introducing external textual\ninformation Classifier Embeddings. ViLD [17] upgrades\nlanguage embeddings to CLIP [28] and extracts regional\nfeatures from CLIP image features. Detic [49] also adopts\nCLIP as a classifier and uses additional image-level data for\njoint training, dramatically expanding the number of cate-\ngories and performance of detection-level tasks. This paper\nuses the Detic detector for the BGS algorithm.\n33. Method\n3.1. Overview\nZBS is among the novel unsupervised BGS algorithms\nfor real applications. It is a zero-shot object detection based\non the model that can obtain an instance-level background\nmodel. ZBS contains three stages: all-instance detection,\ninstance-level background modeling, and foreground in-\nstance selection. Figure 2 illustrates the framework of our\nmethod. First, ZBS uses an all-instance detection model\nto acquire the structured-instance representation. Then, an\ninstance-level background model is built and maintained\nthrough the movement information of instances. Finally,\nwhen a new frame comes, we will select the moving fore-\nground from the detector outputs based on the background\nmodel. "}, {"title": "Section 0097", "content": "We convert the result into a binary mask to compare\nwith other BGS methods.\nAlgorithm 1 : The ZBS algorithm process.\n1:Initialize the zero-shot detector as Z\n2:Initialize the background model as M\n3:while current frame Itis valid do\n4:Stage 1: All-instance detection\n5: output the result Rt\u2190 Z (It)\n6:Stage 2: Instance-level background model\n7: get the track of each instance from bt(part of Rt)\n8: calculate the IoU minofbi\n0\u00b7\u00b7\u00b7bi\ntandbi\n9: update Mbased on IoU minand\u03c4move\n10:Stage 3: Foreground instance selection\n11: separate Mandbtby instance-id\n12: calculate the IoU and IoF of Mandbi\nt\n13: get a binary mask Dt(x)based on IoU &IoF and\n\u03c4fore\n14: current frame \u2190next frame\nend\n3.2. All-instance Detection\nThe goal of background subtraction is to extract all mov-\ning objects as foreground in each video frame. Traditional\nunsupervised BGS methods rely on pixel-level background\nmodels, which struggle to differentiate noisy backgrounds\nfrom foreground objects. To address this, we propose an\ninstance-level background model. It utilizes an instance de-\ntector to locate the objects of all possible categories and all\nlocations in the image and convert the raw image pixels into\nstructured instance representation.\nIntuitively, most existing trained instance segmentation\nnetworks can be used. Besides, the categories of the train-\ning datasets adapt to most domain-adapted object detection\nscenarios. "}, {"title": "Section 0109", "content": "However, instance segmentation networks can-\nnot detect and segment the objects of categories outside the\ntraining datasets\u2019 annotations.Recently, with the development of self-supervised train-\ning and the foundation models [28], several practical zero-\nshot object detection methods have been proposed [17, 49].\nThese methods can detect almost thousands of categories\nof objects without being trained on the applied scenarios.\nTherefore, to obtain a more general and generalized deep\nlearning background modeling method, we adopt the zero-\nshot object detection method Detic [49] as the detector of\nour BGS method. Detic [49] can detect 21k categories of\nobjects and segment the object\u2019s masks.\nDistinguished from the instance segmentation, we call\nthis process all-instance detection . After the all-instance\ndetection stage, the video frame Itis structured by zero-\nshot detector Zas instance representation Rt. The repre-\nsentation Rtincludes instance boxes bi\ntand segmentation\nmasks mi\ntwith category labels ci\nt, where iis the idof the\ninstance, and trefers to the t-th frame of the video.\n3.3. Instance-level Background Modeling\nBased on the all-instance detection results, the algorithm\nshould distinguish which objects have moved and which\nhave not. The ideal instance-level background model should\nbe the collection of all stationary object instances. It is the\nbasis for foreground instance selection. We define the back-\nground model Mas:\nM=n\nbi,bj, . . . ,bko\n(1)\nThe instance-level background model Mis a collection\nof detection boxes for static instances, bi,bj,bkare static\ninstances with id=i, j, k , and the value is the average of\nthe coordinates of all boxes in the past trajectory of this in-\nstance. It reflects which locations have stationary instances\nand which are background without instances. "}, {"title": "Section 0122", "content": "As shown\nin Figure 2 (b), our method uses the initial frames to ob-\ntain an initial instance-level background model and update\nthe background model with a certain period in subsequent\nframes ( \u2206T= 100 is chosen in this paper).\nThe details are shown in Algorithm 1. There are three\nsteps for the instance-level background modeling stage.\nFirst, the proposed method utilizes the detector Zoutput\nboxes bi\ntfrom past frames to obtain the tracks of each in-\nstance (tracks are obtained by SORT method [5]). Sec-\nond, ZBS computes the average value of the coordinates\nfor the upper-left and lower-right corners of each bound-\ning box within the corresponding trajectory of the instance,\ndenoted as bi. Then we can obtain the minimum value of\nIoU of bi\ntandbi, which means the maximum movement be-\ntween the positions compared to the average in the whole\ntrajectory. In our implementation, we apply a median fil-\nter to the trajectory IoU. This helps mitigate abrupt changes\nin IoU caused by object occlusion. Experiments in Table 2\ndemonstrate that this improvement is beneficial. Finally, the\n4update strategy of the instance-level background model is as\nEquation (2):\nM=(\nM \u222a bi, if IoU min(bi\nt,bi)\u2a7e\u03c4move\nM \u2212 (M \u2229 bi), otherwise.(2)\nwhere bi\ntdenotes the i-th instance in t-th image frame. bi\ndenotes the average of all boxes bifor each instance. \u03c4move\nis the threshold for judging whether the instance is moving.\nIf it remains stationary, put it into the background model\nM; otherwise, remove it from the background model M.\nImplementation Details. To build a more robust back-\nground model, we choose a smaller \u03c4conf1in the instance-\nlevel background modeling stage, which helps more station-\nary instances to be incorporated into the background model\nM, it is called \u2206conf.\n3.4. Foreground Instance Selection\nThe key to foreground instance selection is accurately\njudging whether the instance has moved compared to the\nbackground model. Object occlusion is a common chal-\nlenge. "}, {"title": "Section 0134", "content": "When objects are occluded, the object behind them\ncan easily be misjudged as moving. To balance sensitivity\nand robustness to object occlusion, we introduce IoF (In-\ntersection over Foreground) as a complement to IoU (Inter-\nsection over Union) , which is calculated as Equation (3):\nIoF=bi\nrec\u2229bi\nbg\nbirec. (3)\nwhere bi\nrecdenotes the instance in the recent frame, bi\nbg\ndenotes the i-th instance in the instance-level background\nmodelM.\nFigure 3 shows how IoF works. If the instance is not\nmoving but is obscured, IoU drops considerably while IoF\nstill preserves a high value. By judging the two metrics\ntogether, it is robust to determine whether a certain instance\nis a foreground.\nAs shown in the foreground instance selection stage of\nAlgorithm 1, the detection result of each current frame is\nmatched with the set of object instances contained in the\ninstance-level background model. The proposed method\nuses two metrics, IoU and IoF, to determine whether the\ninstance can be used as a foreground. If both IoU and IoF\nare smaller than the foreground selection threshold \u03c4fore, the\ninstance is new or has moved and should be considered as\nforeground. On the contrary, if either IoU or IoF is larger\nthan the threshold \u03c4fore, the instance should not be consid-\nered as foreground. The rule of foreground instance selec-\n1\u03c4confis a score threshold from [49] for the outputs of the all-instance\ndetection stage. This threshold determines the confidence level of the out-\nputs in the first stage.\n\u2229\n\u222aIoU =               \u2248        is small IoF =               \u2248        is large\u2229\nforeground detection  \nusing IoUforeground detection  \nusing IoFnew frame detection result instance-level background modelFigure 3. A typical case for object obstruction. A person half ob-\nscures the bicycle on the left side of the image. "}, {"title": "Section 0144", "content": "The IoU of bbicycleirec\nandbbicyclei\nbg is very small, while the IoF of bbicycleirec andbbicyclei\nbg is\nstill large.\ntion can be expressed by Equation (4).\nDt(x) =\uf8f1\n\uf8f2\n\uf8f3FG, if IoU (bi\nrec, bi\nbg)< \u03c4 fore\nand IoF (bi\nrec, bi\nbg)< \u03c4 fore;\nBG, otherwise .(4)\nwhere Dt(x)is regarded as the x-th instance in t-th frame\nwhether should be a foreground, FG means foreground,\nBGmeans background. bi\nrecdenotes the box of i-th instance\nin recent frame. bi\nbgdenotes the box of foreground instance\nin the instance-level background model M.\n4. Experiments\n4.1. Dataset and Evaluation Metrics\nWe evaluate the performance of the proposed method on\nthe CDnet 2014 dataset [40]. The CDnet 2014 dataset is\nthe most famous benchmark for change detection, includ-\ning 53 video sequences and 11 categories corresponding to\ndifferent classic application scenarios. The main categories\nof the dataset include Bad Weather ,Baseline ,Camera Jit-\nter,Dynamic Background ,Intermittent Object Motion ,Low\nFramerate ,Night videos ,Pan\u2013Tilt\u2013Zoom ,Shadow ,Ther-\nmal, and Turbulence . Ground Truth annotated manually is\navailable for every frame in video sequences and tells us\nwhether each pixel belongs to the background or the fore-\nground. Specifically, in the Ground Truth, Static, Shadow,\nNon-Region of Interest (Non-ROI), Unknown, and Moving\npixels are assigned respectively to grayscale values 0, 50,\n85, 170, and 255. We select Recall ( Re), Precision ( Pr) and\n5F-Measure ( F\u2212M) to evaluate the performance on the CD-\nnet 2014 dataset.\nFollowing [16], we regard Static and Shadow pixels as\nnegative samples (background), regard Moving pixels as\npositive samples (foreground), and ignore Non-ROI and\nUnknown pixels to ensure the fairness of the metrics.\n4.2. "}, {"title": "Section 0157", "content": "Hyper-parameter Sensitivity Analysis\nAs mentioned earlier, our method requires setting several\nparameters before use: the threshold for all-instance detec-\ntion\u03c4conf, the threshold for background modeling \u03c4move, and\nthe threshold for foreground selection \u03c4fore.\nDifferent parameters suit different situations. The large\n\u03c4confis better for more straightforward scenarios. The large\n\u03c4move is better for fast motion or low frame rate scenarios.\nThe small \u03c4foreis robust for camera jitter scenarios. The\nperformance with different parameter settings is shown in\nFigure 4. For most scenarios, the sensitivity of the two pa-\nrameters \u03c4move, \u03c4foreis low, and the impact of the changes of\nthese two hyper-parameters on F-Measure fluctuates within\n\u00b11%. When \u03c4foreis equal to 1, the foreground instance se-\nlector treats all new instances as foreground, so the preci-\nsion and F-Measure have a big drop. \u03c4confdetermines the\noutput of the zero-shot detector, which is very important for\nthe subsequent two stages, and different scenes often ap-\nply to different thresholds. The universal parameters are\n\u03c4conf= 0.6, \u03c4move= 0.5, \u03c4fore= 0.8in Experiments.\n4.3. Quantitative Results\nTable 1 shows the comparison of our method among\nother BGS algorithms, in which F-Measure is observed.\nThese algorithms could be classified into two parts: su-\npervised and unsupervised algorithms. Most supervised al-\ngorithms, such as FgSegNet [24] and CascadeCNN [41],\nhave nearly perfect F-Measure because they are trained with\nsome frames in test videos. However, the performance\nof these methods decreases significantly when applied to\nunseen videos because of the lack of generalization abil-\nity. FgSegNet in unseen videos only achieves 0.3715 F-\nMeasure. STPNet [42] and BSUV-Net 2.0 [37] are super-\nvised algorithms designed explicitly for unseen videos and\ncan achieve F-Measure of around 0.8. "}, {"title": "Section 0169", "content": "IUTIS-5 [6] is a spe-\ncial supervised algorithm that learns how to combine vari-\nous unsupervised algorithms from datasets.\nThe remaining methods are unsupervised algorithms [19,\n22, 34, 35, 43] which naturally can handle unseen videos.\nThe results show that our method outperforms all unsu-\npervised algorithms. In particular, ZBS outperforms the\nstate-of-the-art RT-SBS-v2 [13] by 4.70 %. Moreover, ZBS\noutperforms supervised method in unseen videos BSUV-\nNet 2.0 [37]. When considering per-category F-Measure,\nour method has advantages in seven out of eleven cate-\ngories, such as Camera Jitter ,Intermittent Object Motion ,andNight Videos . However, our method cannot deal with\nTurbulence well because the detector of the all-instance de-\ntection module cannot adapt to the unnatural image distri-\nbution of Turbulence scenarios without training.\n4.4. Ablation Study\nAblation experiments are conducted, in which we add\nthe ablation components one by one to measure their effec-\ntiveness. The results are summarized in Table 2 with preci-\nsion, recall, and F-Measure.\nThe baseline is to use the result of the all-instance detec-\ntor directly as the foreground. In the instance-level back-\nground modeling stage, we only build an instance-level\nbackground model, but do not judge whether the foreground\nis moving. The performance of the algorithm is slightly\nimproved and exceeds the baseline. In the foreground se-\nlection stage, the algorithm uses the background model to\ndetermine whether the foreground is moving. The perfor-\nmance is greatly improved. Moreover, we propose three\nmodules to enhance the instance-level background model.\nAfter adding them to the algorithm one by one, the algo-\nrithm\u2019s performance larger gains and the advantages of the\ninstance-level background model are fully demonstrated.\nTable 2 shows that simple instance-level background and\nforeground capture most potential moving objects. "}, {"title": "Section 0185", "content": "The for-\nmer exhibits higher recall but slightly lower precision than\npixel-level background and foreground. \u2206conf enhances\noverall performance. Object occlusion impacts the back-\nground model and complicates foreground selection. This\nissue is unique to instance-level representation. We propose\nthe \u201dIoU filter\u201d and \u201dIoF\u201d to mitigate this problem, both of\nwhich reduce false positives, particularly \u201dIoF\u201d.\n4.5. More Analysis\nVisual Result. A visual comparison from different methods\nis shown in Figure 5. It includes six challenging scenarios\nfrom the CDnet 2014 dataset. In the \u201dhighway\u201d scenario,\nthe main challenge is the shadow of the car and the tree\nbranches moving with the wind. Because of the instance-\nlevel foreground detection, our method is robust to noisy\nbackground regions. In the \u201dboulevard\u201d scenario, affected\nby the jitter of the camera, many false positives are pro-\nduced by other BGS methods. However, our method is ro-\nbust to camera shake due to the instance-level background\nmodel. In the \u201dboats\u201d scenario, GMM and SuBSENSE pro-\nduce many false positives because of water rippling. ZBS\ncan extract clear detection results within the background\ndisturbance. In the \u201dsofa\u201d scenario, which contains inter-\nmittent object motion, the proposed method has better seg-\nmentation results at the contour regions of objects. In the\n\u201dpeopleInShade\u201d scenario, ZBS excels in shadow regions.\nDespite challenges in the \u201dcontinuousPan\u201d scenario, our\nmethod remains robust. "}, {"title": "Section 0188", "content": "These results highlight the advan-\n60.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.850.600.650.700.750.800.850.900.951.00Precision | Recall | F-Measure(%)\nPrecision\nRecall\nF-Measure(a) The sensitivity of the threshold \u03c4conf\n0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.80.600.650.700.750.800.850.900.951.00Precision | Recall | F-Measure(%)\nPrecision\nRecall\nF-Measure (b) The sensitivity of the threshold \u03c4move\n0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1.00.600.650.700.750.800.850.900.951.00Precision | Recall | F-Measure(%)\nPrecision\nRecall\nF-Measure (c) The sensitivity of the threshold \u03c4fore\nFigure 4. The hyper-parameter sensitivity analysis. The relationship between the thresholds ( \u03c4conf, \u03c4move, \u03c4fore) and the evaluation metrics\n(Precision, Recall, F-Measure) of the BGS algorithms.\nTable 1. "}, {"title": "Section 0192", "content": "Overall and per-category F-Measure comparison of different BGS methods on the CDnet 2014 dataset.\nMethod baseline camjitt dynbg intmot shadow thermal badwea lowfr night PTZ turbul Overall\nSupervised algorithms\nCascadeCNN [41] 0.9786 0.9758 0.9658 0.8505 0.9593 0.8958 0.9431 0.8370 0.8965 0.9168 0.9108 0.9209\nMU-Net2 [30] 0.9900 0.9824 0.9892 0.9894 0.9845 0.9842 0.9343 0.8706 0.8362 0.8185 0.9272 0.9369\nBSPVGAN [47] 0.9837 0.9893 0.9849 0.9366 0.9849 0.9764 0.9644 0.8508 0.9001 0.9486 0.9310 0.9501\nFgSegNetv2 [25] 0.9978 0.9971 0.9951 0.9961 0.9955 0.9938 0.9904 0.9336 0.9739 0.9862 0.9727 0.9847\nFgSegNet [24]\n(unseen video)0.6926 0.4266 0.3634 0.2002 0.5295 0.6038 0.3277 0.2482 0.2800 0.3503 0.0643 0.3715\nSTPNet [42] 0.9587 0.7721 0.8058 0.8267 0.9114 0.8688 0.8898 0.7297 0.6961 0.6076 0.7248 0.7992\nBSUV-Net 2.0 [37] 0.9620 0.9004 0.9057 0.8263 0.9562 0.8932 0.8844 0.7902 0.5857 0.7037 0.8174 0.8387\nIUTIS-5 [6] 0.9567 0.8332 0.8902 0.7296 0.8766 0.8303 0.8248 0.7743 0.5290 0.4282 0.7836 0.7717\nUnsupervised algorithms\nPAWCS [35] 0.9397 0.8137 0.8938 0.7764 0.8913 0.8324 0.8152 0.6588 0.4152 0.4615 0.6450 0.7403\nSuBSENSE [34] 0.9503 0.8152 0.8177 0.6569 0.8986 0.8171 0.8619 0.6445 0.5599 0.3476 0.7792 0.7408\nWisenetMD [22] 0.9487 0.8228 0.8376 0.7264 0.8984 0.8152 0.8616 0.6404 0.5701 0.3367 0.8304 0.7535\nSWCD [19] 0.9214 0.7411 0.8645 0.7092 0.8779 0.8581 0.8233 0.7374 0.5807 0.4545 0.7735 0.7583\nSemanticBGS [9] 0.9604 0.8388 0.9489 0.7878 0.9478 0.8219 0.8260 0.7888 0.5014 0.5673 0.6921 0.7892\nRTSS [43] 0.9597 0.8396 0.9325 0.7864 0.9551 0.8510 0.8662 0.6771 0.5295 0.5489 0.7630 0.7917\nRT-SBS-v2 [13] 0.9535 0.8233 0.9217 0.8946 0.9497 0.8697 0.8279 0.7341 0.5629 0.5808 0.7315 0.8045\nZBS (Ours) 0.9653 0.9545 0.9290 0.8758 0.9765 0.8698 0.9229 0.7433 0.6800 0.8133 0.6358 0.8515\nTable 2. Ablation of the three stages of our methods and other\nimprovements. AID: All-instance detection (Section 3.2). IBM:\nInstance-level background modeling (Section 3.3). "}, {"title": "Section 0210", "content": "FIS: Fore-\nground instance selection (Section 3.4). \u2206conf: Different con-\nfidence thresholds for background modeling and foreground se-\nlection. Filter: Median filtering on movement information. IoF:\nIntersection over Foreground measurement standard.\nAID IBM FISEnhancePr Re F-M\u2206conf Filter IoF\n\u2714 0.4076 0.8869 0.4980\n\u2714 \u2714 0.5343 0.8022 0.5752\n\u2714 \u2714 \u2714 0.7468 0.7625 0.7152\n\u2714 \u2714 \u2714 \u2714 0.7529 0.7851 0.7415\n\u2714 \u2714 \u2714 \u2714 \u2714 0.8249 0.7829 0.7836\n\u2714 \u2714 \u2714 \u2714 \u2714 \u2714 0.8802 0.8403 0.8515\ntages of our instance-level background-based ZBS.\nRuntime Efficiency. Achieving real-time performance is\nvital for BGS algorithms. The main time-consuming aspect\nof ZBS is concentrated in the first stage, which involves pre-training the zero-shot object detection model. We have im-\nplemented the two subsequent stages in C++. The FPS is\nabout 20 on one A100 GPU. In the all-instance detection\nstage, we used parallel computation with a batch size of 8\nand processed results sequentially in the subsequent stages.\nUltimately, we achieve approximately 44 FPS on an A100\nGPU.\nMoreover, adopting TensorRT SDK [39], quantization,\nand frame skipping can further improves the FPS in real-\nworld applications. However, this paper mainly focus on\nenhancing the accuracy of BGS (F-Measure). In future\nstudies, we plan to further improve its runtime efficiency.\nPerformance in complex scenarios. To further demon-\nstrate the good performance of ZBS in complex scenar-\nios, we compare the nbShadowError and FPR-S of differ-\nent BGS methods in the Shadow category. FPR-S is calcu-\nlated as Equation (5). Table 3 shows that our method has\nan extremely low false positive rate on shadow pixels. ZBS\n7Ground T ruthInput Frame\nGMM\nRTSSSuBSENSE+PSPNetBSUV -Net 2.0\nZBS (Ours)\n(a) (b) (c) (d) (e) (f)\nSuBSENSE\nRT-SBS-v2Figure 5. Comparison of the visual results on various scenarios from CDnet 2014 dataset. Except for our method, other segmentation\nresults are quoted in RTSS [43]. From left to right: (a) Scenario \u201dhighway\u201d from the Baseline category. "}, {"title": "Section 0220", "content": "(b) Scenarios \u201dboulevard\u201d from\ntheCamera Jitter category. (c) Scenario \u201dboats\u201d from the Dynamic Background category. (d) Scenario \u201dsofa\u201d from the Intermittent Object\nMotion category. (e) Scenario \u201dpeopleInShade\u201d from the Shadow category. (f) Scenario \u201dcontinuousPan\u201d from the PTZ category.\nfar outperforms all unsupervised BGS methods and is better\nthan the state-of-the-art supervised method FgSegNet [24].\nIn the appendix, we also add experiments to demonstrate\nthe good performance in other complex scenes such as night\nlight, camouflaged foreground, etc.\nFPR-S =nbShadowError /nbShadow (5)\nwhere nbShadowError is the number of times a pixel is la-\nbeled as shadow in Ground Truth but detected as a moving\nobject. nbShadow is the total number of pixels labeled as a\nshadow in Ground Truth for a video or category.\nTable 3. The FPR-S and nbShadowError of different BGS meth-\nods.\nMethodnbShadowErrorFPR-SbusStation peopleInShade bungalows cubicle\nFgSegNet [24] 2383 12866 5375 580 0.0042\nBSUV-Net 2.0 [37] 23149 564989 982943 111438 0.2506\nSuBSENSE [34] 315658 854157 1705793 391569 0.5996\nSemanticBGS [9] 169426 782489 730668 33137 0.3018\nRT-SBS-v2 [13] 28530 457467 566642 52746 0.1717\nZBS (Ours) 964 1892 10403 390 0.00195. Conclusion\nIn this paper, we propose a novel background sub-\ntraction framework, ZBS, consisting of three components:\nall-instance detection, instance-level background modeling,\nand foreground instance selection. Experiments on the CD-\nnet 2014 dataset show the algorithm\u2019s effectiveness. Com-\npared with other BGS methods, our method achieves state-\nof-the-art performance among all unsupervised BGS meth-\nods and even outperforms many supervised deep learning\nalgorithms. "}, {"title": "Section 0242", "content": "ZBS detects most real-world categories with-\nout pre-defined foreground categories, producing accurate\nforeground edges and reducing false detections.\nOur future work is leveraging instance-level information\nmore effectively and compressing the all-instance detector\nfor better efficiency.\nAcknowledgement. This work was supported by\nNational Key R &D Program of China under Grant\nNo.2021ZD0110403. This work was also supported by Na-\ntional Natural Science Foundation of China under Grants\n61976210, 62176254, 62006230, 62002357, and 62206290.\n8References\n[1] Fateme Bahri and Nilanjan Ray. Dynamic background sub-\ntraction by generative neural networks. In 2022 18th IEEE\nInternational Conference on Advanced Video and Signal\nBased Surveillance (AVSS) , pages 1\u20138. IEEE, 2022. 2\n[2] Mohammed Chafik Bakkay, Hatem A Rashwan, Houssam\nSalmane, Louahdi Khoudour, D Puig, and Yassine Ruichek.\nBSCGAN: Deep background subtraction with conditional\ngenerative adversarial networks. In 2018 25th IEEE Interna-\ntional Conference on Image Processing (ICIP) , pages 4018\u2013\n4022. IEEE, 2018. 2\n[3] Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chel-\nlappa, and Ajay Divakaran. Zero-shot object detection. In\nProceedings of the European Conference on Computer Vi-\nsion (ECCV) , pages 384\u2013400, 2018. 3\n[4] Olivier Barnich and Marc Van Droogenbroeck. ViBe:\nA universal background subtraction algorithm for video\nsequences. IEEE Transactions on Image processing ,\n20(6):1709\u20131724, 2010. 1\n[5] Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and\nBen Upcroft. Simple online and realtime tracking. In 2016\nIEEE international conference on image processing (ICIP) ,\npages 3464\u20133468. IEEE, 2016. 4\n[6] Simone Bianco, Gianluigi Ciocca, and Raimondo Schettini.\nCombination of video change detection algorithms by ge-\nnetic programming. IEEE Transactions on Evolutionary\nComputation , 21(6):914\u2013928, 2017. 6, 7\n[7] Thierry Bouwmans. "}, {"title": "Section 0270", "content": "Traditional and recent approaches\nin background modeling for foreground detection: An\noverview. Computer science review , 11:31\u201366, 2014. 1\n[8] Thierry Bouwmans, Sajid Javed, Maryam Sultana, and\nSoon Ki Jung. Deep neural network concepts for background\nsubtraction: A systematic review and comparative evalua-\ntion. Neural Networks , 117:8\u201366, 2019. 2\n[9] Marc Braham, Sebastien Pierard, and Marc Van Droogen-\nbroeck. Semantic background subtraction. In 2017 IEEE In-\nternational Conference on Image Processing (ICIP) , pages\n4552\u20134556. Ieee, 2017. 2, 7, 8\n[10] Marc Braham and Marc Van Droogenbroeck. Deep back-\nground subtraction with scene-specific convolutional neural\nnetworks. In 2016 international conference on systems, sig-\nnals and image processing (IWSSIP) , pages 1\u20134. IEEE, 2016.\n2\n[11] Yingying Chen, Jinqiao Wang, Bingke Zhu, Ming Tang, and\nHanqing Lu. Pixelwise deep sequence learning for moving\nobject detection. IEEE Transactions on Circuits and Systems\nfor Video Technology , 29(9):2567\u20132579, 2017. 2\n[12] Pojala Chiranjeevi and S Sengupta. New fuzzy texture fea-\ntures for robust detection of moving objects. IEEE Signal\nProcessing Letters , 19(10):603\u2013606, 2012. 1\n[13] Anthony Cioppa, Marc Van Droogenbroeck, and Marc Bra-\nham. Real-time semantic background subaction. In 2020\nIEEE International Conference on Image Processing (ICIP) ,\npages 3214\u20133218. IEEE, 2020. 2, 6, 7, 8\n[14] Ahmed Elgammal, Ramani Duraiswami, David Harwood,\nand Larry S Davis. Background and foreground model-\ning using nonparametric kernel density estimation for visualsurveillance. Proceedings of the IEEE , 90(7):1151\u20131163,\n2002. 1\n[15] Belmar Garcia-Garcia, Thierry Bouwmans, and Alberto\nJorge Rosales Silva. Background subtraction in real appli-\ncations: Challenges, current models and future directions.\nComputer Science Review , 35:100204, 2020. 1\n[16] Nil Goyette, Pierre-Marc Jodoin, Fatih Porikli, Janusz Kon-\nrad, and Prakash Ishwar. "}, {"title": "Section 0300", "content": "Changedetection.net: A new\nchange detection benchmark dataset. In 2012 IEEE computer\nsociety conference on computer vision and pattern recogni-\ntion workshops , pages 1\u20138. IEEE, 2012. 6\n[17] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.\nOpen-vocabulary object detection via vision and language\nknowledge distillation. arXiv preprint arXiv:2104.13921 ,\n2021. 3, 4\n[18] Enqiang Guo, Xinsha Fu, Jiawei Zhu, Min Deng, Yu\nLiu, Qing Zhu, and Haifeng Li. Learning to measure\nchange: Fully convolutional siamese metric networks for\nscene change detection. arXiv preprint arXiv:1810.09111 ,\n2018. 2\n[19] Sahin Isik, Kemal \u00a8Ozkan, Serkan G \u00a8unal, and \u00a8Omer Nezih\nGerek. SWCD: a sliding window and self-regulated\nlearning-based background updating method for change\ndetection in videos. Journal of Electronic Imaging ,\n27(2):023002, 2018. 6, 7\n[20] S. Jabri, Zoran Duric, Harry Wechsler, and Azriel Rosen-\nfeld. Detection and location of people in video images using\nadaptive fusion of color and edge information. International\nConference on Pattern Recognition , 2000. 1\n[21] Kyungnam Kim, Thanarat H Chalidabhongse, David Har-\nwood, and Larry Davis. Background modeling and subtrac-\ntion by codebook construction. In ICIP04 , pages 3061\u20133064,\n2004. 1\n[22] Sang-ha Lee, Gyu-cheol Lee, Jisang Yoo, and Soonchul\nKwon. Wisenetmd: Motion detection using dynamic back-\nground region analysis. Symmetry , 11(5):621, 2019. 6, 7\n[23] Zhihui Li, Lina Yao, Xiaoqin Zhang, Xianzhi Wang, Salil S.\nKanhere, and Huaxiang Zhang. Zero-shot object detection\nwith textual descriptions. national conference on artificial\nintelligence , 2019. 3\n[24] Long Ang Lim and Hacer Yalim Keles. Foreground segmen-\ntation using convolutional neural networks for multiscale\nfeature encoding. Pattern Recognition Letters , 112:256\u2013262,\n2018. 1, 2, 6, 7, 8\n[25] Long Ang Lim and Hacer Yalim Keles. Learning multi-scale\nfeatures for foreground segmentation. Pattern Analysis and\nApplications , 23(3):1369\u20131380, 2020. "}, {"title": "Section 0324", "content": "7\n[26] Kevin Lin, Shen-Chi Chen, Chu-Song Chen, Daw-Tung Lin,\nand Yi-Ping Hung. Abandoned object detection via temporal\nconsistency modeling and back-tracing verification for visual\nsurveillance. IEEE Transactions on Information Forensics\nand Security , 2015. 11\n[27] Jeffrey Pennington, Richard Socher, and Christopher D Man-\nning. Glove: Global vectors for word representation. In\nProceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP) , pages 1532\u20131543,\n2014. 3\n9[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning ,\npages 8748\u20138763. PMLR, 2021. 3, 4\n[29] Shafin Rahman, Salman Khan, and Nick Barnes. Improved\nvisual-semantic alignment for zero-shot object detection. In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence , pages 11932\u201311939, 2020. 3\n[30] Gani Rahmon, Filiz Bunyak, Guna Seetharaman, and Kan-\nnappan Palaniappan. Motion U-Net: Multi-cue encoder-\ndecoder network for motion segmentation. In 2020 25th\nInternational Conference on Pattern Recognition (ICPR) ,\npages 8125\u20138132. IEEE, 2021. 7\n[31] Dimitrios Sakkos, Heng Liu, Jungong Han, and Ling Shao.\nEnd-to-end video background subtraction with 3d convolu-\ntional neural networks. Multimedia Tools and Applications ,\n77(17):23023\u201323041, 2018. 2\n[32] Marcos CS Santana, Leandro Aparecido Passos, Thierry P\nMoreira, Danilo Colombo, Victor Hugo C de Albuquerque,\nand Joao Paulo Papa. A novel siamese-based approach\nfor scene change detection with applications to obstructed\nroutes in hazardous environments. IEEE Intelligent Systems ,\n35(1):44\u201353, 2019. 2\n[33] Bruno Sauvalle and Arnaud de La Fortelle. Autoencoder-\nbased background reconstruction and foreground segmen-\ntation with background noise estimation. "}, {"title": "Section 0354", "content": "arXiv preprint\narXiv:2112.08001 , 2021. 2\n[34] Pierre-Luc St-Charles, Guillaume-Alexandre Bilodeau, and\nRobert Bergevin. SuBSENSE: A universal change detection\nmethod with local adaptive sensitivity. IEEE Transactions\non Image Processing , 24(1):359\u2013373, 2014. 1, 6, 7, 8\n[35] Pierre-Luc St-Charles, Guillaume-Alexandre Bilodeau, and\nRobert Bergevin. A self-adjusting approach to change de-\ntection based on background word consensus. In 2015 IEEE\nwinter conference on applications of computer vision , pages\n990\u2013997. IEEE, 2015. 1, 6, 7\n[36] Chris Stauffer and W Eric L Grimson. Adaptive background\nmixture models for real-time tracking. In Proceedings. 1999\nIEEE computer society conference on computer vision and\npattern recognition (Cat. No PR00149) , volume 2, pages\n246\u2013252. IEEE, 1999. 1\n[37] M Ozan Tezcan, Prakash Ishwar, and Janusz Konrad. BSUV-\nNet 2.0: Spatio-temporal data augmentations for video-\nagnostic supervised background subtraction. IEEE Access ,\n9:53849\u201353860, 2021. 2, 6, 7, 8\n[38] Ozan Tezcan, Prakash Ishwar, and Janusz Konrad. BSUV-\nNet: A fully-convolutional neural network for background\nsubtraction of unseen videos. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision , pages 2774\u20132783, 2020. 2\n[39] Han Vanholder. Efficient inference with tensorrt. In GPU\nTechnology Conference , volume 1, page 2, 2016. 7\n[40] Yi Wang, Pierre-Marc Jodoin, Fatih Porikli, Janusz Konrad,\nYannick Benezeth, and Prakash Ishwar. CDnet 2014: An\nexpanded change detection benchmark dataset. In Proceed-ings of the IEEE conference on computer vision and pattern\nrecognition workshops , pages 387\u2013394, 2014. 5\n[41] Yi Wang, Zhiming Luo, and Pierre-Marc Jodoin. Interactive\ndeep learning method for segmenting moving objects. Pat-\ntern Recognition Letters , 96:66\u201375, 2017. 2, 6, 7\n[42] Yizhong Yang, Jiahao Ruan, Yongqiang Zhang, Xin Cheng,\nZhang Zhang, and Guangjun Xie. "}, {"title": "Section 0382", "content": "STPNet: A spatial-\ntemporal propagation network for background subtraction.\nIEEE Transactions on Circuits and Systems for Video Tech-\nnology , 32(4):2145\u20132157, 2021. 2, 6, 7\n[43] Dongdong Zeng, Xiang Chen, Ming Zhu, Michael Goesele,\nand Arjan Kuijper. Background subtraction with real-time\nsemantic segmentation. IEEE Access , 7:153869\u2013153884,\n2019. 2, 6, 7, 8\n[44] Hongxun Zhang and De Xu. Fusing color and texture fea-\ntures for background model. In Fuzzy Systems and Knowl-\nedge Discovery: Third International Conference, FSKD\n2006, Xi\u2019an, China, September 24-28, 2006. Proceedings 3 ,\npages 887\u2013893. Springer, 2006. 1\n[45] Chenqiu Zhao, Kangkang Hu, and Anup Basu. Univer-\nsal background subtraction based on arithmetic distribution\nneural network. IEEE Transactions on Image Processing ,\n31:2934\u20132949, 2022. 2\n[46] Xu Zhao, Yingying Chen, Ming Tang, and Jinqiao Wang.\nJoint background reconstruction and foreground segmenta-\ntion via a two-stage convolutional neural network. In 2017\nIEEE International Conference on Multimedia and Expo\n(ICME) , pages 343\u2013348. IEEE, 2017. 2\n[47] Wenbo Zheng, Kunfeng Wang, and Fei-Yue Wang. A novel\nbackground subtraction algorithm based on parallel vision\nand bayesian gans. Neurocomputing , 394:178\u2013200, 2020. 2,\n7\n[48] Dongxiang Zhou and Hong Zhang. Modified gmm back-\nground modeling and optical flow for detection of moving\nobjects. In 2005 IEEE international conference on systems,\nman and cybernetics , volume 3, pages 2224\u20132229. IEEE,\n2005. 1\n[49] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp\nKr\u00a8ahenb \u00a8uhl, and Ishan Misra. Detecting twenty-thousand\nclasses using image-level supervision. In Computer Vision\u2013\nECCV 2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23\u201327, 2022, Proceedings, Part IX , pages 350\u2013368.\nSpringer, 2022. 2, 3, 4, 5\nA. Instance-level foreground detection\nUnlike previous methods, our method builds an instance-\nlevel background model. Therefore, ZBS can achieve\ninstance-level foreground detection. "}, {"title": "Section 0395", "content": "Figure 6 shows the dif-\nference between binary foreground detection and instance-\nlevel foreground detection. Figure 6b shows that ZBS can\ndetect moving foreground of different granularities, includ-\ning person, backpack, shoe, beanie, etc., and can correctly\nclassify stationary subway and crossbar as background.\n10(a) Binary foreground detec-\ntion.\n(b) Instance-level foreground\ndetection.\nFigure 6. The binary and instance-level foreground detection of\nZBS. Our method can detect the moving foreground of different\ngranularities.\nB. Abandoned object detection\nAbandoned object detection in video surveillance is crit-\nical for ensuring public safety and is a crucial component\nof Intelligent Monitoring. This task presents a challenge,\nas the categories of abandoned objects are highly diverse\nand difficult to learn through traditional supervised training\nmethods. Traditional background subtraction techniques\noften prove insufficient in addressing this issue. Our pro-\nposed method, however, offers a solution by incorporating\na stronger semantic discernment and instance-level back-\nground model, resulting in effective detection of abandoned\nobjects.\nTo adapt to new tasks, we have added a new rule that\nconsiders both motion information and the relationships be-\ntween instances. If an object exhibits isolated, static be-\nhavior or moves independently after previously moving in\nsync with categories such as a person or car, the instance\nis deemed to be an abandoned object. This straightforward\nsemantic rule has proven to be effective in diverse environ-\nments. We have conducted thorough experiments on the\npublic datasets PETS2006 and ABODA, as well as a non-\npublic traffic abandoned object detection dataset known as\nTADA.\nB.1. PETS2006\nThe PETS2006 dataset includes sequences from seven\ndifferent scenes. Each sequence contains an abandonment\nevent except for the third event. "}]]